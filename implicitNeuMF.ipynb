{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec3c3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Callable, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3cd7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sids = df['sid'].values.astype(np.int64)\n",
    "        self.pids = df['pid'].values.astype(np.int64)\n",
    "        self.ratings = df['rating'].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sids[idx], self.pids[idx], self.ratings[idx]\n",
    "\n",
    "    \n",
    "class ImplicitFeedbackDataset(Dataset):\n",
    "    \"\"\"Dataset for both explicit ratings and implicit feedback\"\"\"\n",
    "    def __init__(self, explicit_df, implicit_df=None, author_map=None, venue_map=None):\n",
    "        self.sids = explicit_df['sid'].values.astype(np.int64)\n",
    "        self.pids = explicit_df['pid'].values.astype(np.int64)\n",
    "        \n",
    "        # Get ratings from explicit feedback\n",
    "        self.ratings = explicit_df['rating'].values.astype(np.float32)\n",
    "        \n",
    "        # Get implicit feedback (1 for items in wishlist, 0 otherwise)\n",
    "        if implicit_df is not None:\n",
    "            # Create mapping of (sid, pid) to implicit feedback\n",
    "            implicit_map = {(row['sid'], row['pid']): 1 for _, row in implicit_df.iterrows()}\n",
    "            \n",
    "            # Get implicit feedback for each (sid, pid) pair in explicit_df\n",
    "            self.implicit_feedback = np.array([\n",
    "                implicit_map.get((sid, pid), 0) \n",
    "                for sid, pid in zip(self.sids, self.pids)\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            # If no implicit feedback, use zeros\n",
    "            self.implicit_feedback = np.zeros_like(self.ratings)\n",
    "            \n",
    "        # Get author and venue information if provided\n",
    "        if author_map is not None:\n",
    "            self.author_ids = np.array([author_map.get(pid, 0) for pid in self.pids], dtype=np.int64)\n",
    "        else:\n",
    "            self.author_ids = np.zeros_like(self.pids)\n",
    "            \n",
    "        if venue_map is not None:\n",
    "            self.venue_ids = np.array([venue_map.get(pid, 0) for pid in self.pids], dtype=np.int64)\n",
    "        else:\n",
    "            self.venue_ids = np.zeros_like(self.pids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.sids[idx], \n",
    "            self.pids[idx], \n",
    "            self.author_ids[idx], \n",
    "            self.venue_ids[idx],\n",
    "            self.ratings[idx], \n",
    "            self.implicit_feedback[idx]\n",
    "        )\n",
    "    \n",
    "class ImplicitNeuMF(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_users, \n",
    "        num_items, \n",
    "        embedding_dim=64, \n",
    "        mlp_dims=[128, 64, 32], \n",
    "        dropout_rate=0.3\n",
    "    ):\n",
    "        super(ImplicitNeuMF, self).__init__()\n",
    "        \n",
    "        # Base embeddings for both pathways\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "        # MLP pathway\n",
    "        mlp_layers = []\n",
    "        in_size = embedding_dim * 2\n",
    "        for out_size in mlp_dims:\n",
    "            mlp_layers += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.BatchNorm1d(out_size),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ]\n",
    "            in_size = out_size\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # TBR-specific pathway (for incorporating wishlist data)\n",
    "        self.tbr_user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.tbr_item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.explicit_predictor = nn.Linear(mlp_dims[-1], 1)\n",
    "        self.tbr_predictor = nn.Linear(embedding_dim * 2, 1)\n",
    "        \n",
    "        # Learnable weight for balancing explicit and implicit signals\n",
    "        self.implicit_weight = nn.Parameter(torch.tensor(0.2))\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Initialize embeddings\n",
    "        for module in [self.user_embedding, self.item_embedding, \n",
    "                      self.tbr_user_embedding, self.tbr_item_embedding]:\n",
    "            nn.init.normal_(module.weight, std=0.01)\n",
    "        \n",
    "        # Initialize bias\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "        # Initialize linear layers\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.explicit_predictor.weight)\n",
    "        nn.init.xavier_uniform_(self.tbr_predictor.weight)\n",
    "    \n",
    "    def forward(self, user_indices, item_indices, is_eval=False):\n",
    "        # Base embeddings\n",
    "        user_embed = self.user_embedding(user_indices)\n",
    "        item_embed = self.item_embedding(item_indices)\n",
    "        \n",
    "        # MLP for explicit ratings\n",
    "        concat = torch.cat([user_embed, item_embed], dim=1)\n",
    "        mlp_output = self.mlp(concat)\n",
    "        \n",
    "        # Explicit score with bias terms\n",
    "        explicit_score = self.explicit_predictor(mlp_output).squeeze()\n",
    "        u_bias = self.user_bias(user_indices).squeeze()\n",
    "        i_bias = self.item_bias(item_indices).squeeze()\n",
    "        explicit_score = explicit_score + u_bias + i_bias + self.global_bias\n",
    "        \n",
    "        # During evaluation, just use the explicit pathway which should be more reliable\n",
    "        if is_eval:\n",
    "            return torch.clamp(explicit_score, 1.0, 5.0)\n",
    "        \n",
    "        # TBR pathway for implicit feedback\n",
    "        tbr_user = self.tbr_user_embedding(user_indices)\n",
    "        tbr_item = self.tbr_item_embedding(item_indices)\n",
    "        tbr_concat = torch.cat([tbr_user, tbr_item], dim=1)\n",
    "        tbr_score = self.tbr_predictor(tbr_concat).squeeze()\n",
    "        \n",
    "        # Combine with learned weight\n",
    "        weight = torch.sigmoid(self.implicit_weight)  # between 0 and 1\n",
    "        final_score = (1 - weight) * explicit_score + weight * tbr_score\n",
    "        \n",
    "        return torch.clamp(final_score, 1.0, 5.0)\n",
    "    \n",
    "    def predict(self, user_indices, item_indices):\n",
    "        return self.forward(user_indices, item_indices, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25)\n",
    "    return train_df, valid_df\n",
    "\n",
    "# Read wishlist data\n",
    "def read_tbr_data() -> pd.DataFrame:\n",
    "    \"\"\"Reads the to-be-read (wishlist) data.\"\"\"\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))\n",
    "    # No need to split sid_pid since columns are already separate\n",
    "    df[[\"sid\", \"pid\"]] = df[[\"sid\", \"pid\"]].astype(int)\n",
    "    return df\n",
    "\n",
    "def evaluate_model(model, data_loader, device, use_explicit_only=False):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_ids, item_ids, _, _, ratings, _ in data_loader:\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            \n",
    "            predictions = model(user_ids, item_ids, is_eval=use_explicit_only)\n",
    "            \n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_targets.append(ratings.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    rmse = root_mean_squared_error(all_targets, all_preds)\n",
    "    return rmse\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Apply user-specific normalization to ratings\"\"\"\n",
    "    # Get global mean\n",
    "    global_mean = df['rating'].mean()\n",
    "    print(f\"Global mean rating: {global_mean:.4f}\")\n",
    "    \n",
    "    # Get user biases (average rating deviation from global mean)\n",
    "    user_biases = df.groupby('sid')['rating'].mean() - global_mean\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Normalize ratings by user bias\n",
    "    def normalize_rating(row):\n",
    "        user_id = row['sid']\n",
    "        return row['rating'] - user_biases.get(user_id, 0)\n",
    "    \n",
    "    df_norm['rating'] = df_norm.apply(normalize_rating, axis=1)\n",
    "    \n",
    "    return df_norm, user_biases, global_mean\n",
    "\n",
    "def prepare_implicit_feedback(ratings_df, tbr_df, threshold=3.5):\n",
    "    \"\"\"\n",
    "    Prepare implicit feedback from both ratings and TBR datasets\n",
    "    \n",
    "    Args:\n",
    "        ratings_df: DataFrame with explicit ratings\n",
    "        tbr_df: DataFrame with to-be-read items\n",
    "        threshold: Rating threshold to consider as positive feedback (default: 3.5)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with user_id, item_id, and implicit feedback\n",
    "    \"\"\"\n",
    "    # Get positive implicit feedback from high ratings\n",
    "    positive_ratings = ratings_df[ratings_df['rating'] >= threshold][['sid', 'pid']]\n",
    "    positive_ratings['implicit'] = 1\n",
    "    \n",
    "    # Get implicit feedback from TBR items\n",
    "    tbr_implicit = tbr_df[['sid', 'pid']].copy()\n",
    "    tbr_implicit['implicit'] = 1\n",
    "    \n",
    "    # Combine both sources and remove duplicates\n",
    "    implicit_feedback = pd.concat([positive_ratings, tbr_implicit]).drop_duplicates(['sid', 'pid'])\n",
    "    \n",
    "    return implicit_feedback\n",
    "\n",
    "def train_implicit_model(model, train_loader, valid_loader, optimizer, device, \n",
    "                         explicit_criterion=nn.MSELoss(),\n",
    "                         implicit_criterion=nn.BCEWithLogitsLoss(),\n",
    "                         implicit_weight=0.2,  # Weight for implicit loss\n",
    "                         epochs=20, patience=5):\n",
    "    \"\"\"\n",
    "    Training with staged approach, incorporating both explicit and implicit losses\n",
    "    \"\"\"\n",
    "    best_rmse = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    # Stage 1: Train only explicit pathway (5 epochs)\n",
    "    print(\"Stage 1: Training explicit pathway only...\")\n",
    "    \n",
    "    # Create optimizer that only updates explicit pathway parameters\n",
    "    explicit_params = [\n",
    "        model.user_embedding.parameters(),\n",
    "        model.item_embedding.parameters(),\n",
    "        model.user_bias.parameters(), \n",
    "        model.item_bias.parameters(),\n",
    "        model.mlp.parameters(),\n",
    "        model.explicit_predictor.parameters()\n",
    "    ]\n",
    "    explicit_optimizer = torch.optim.Adam(chain(*explicit_params), lr=0.001)\n",
    "    \n",
    "    for epoch in range(1, 6):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            user_ids, item_ids, _, _, ratings, _ = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            \n",
    "            explicit_optimizer.zero_grad()\n",
    "            # Pass is_eval=True to only use explicit pathway\n",
    "            predictions = model(user_ids, item_ids, is_eval=True)\n",
    "            loss = explicit_criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            explicit_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "        val_rmse = evaluate_model(model, valid_loader, device, use_explicit_only=True)\n",
    "        print(f\"Epoch {epoch}/5 (Explicit) - Val RMSE: {val_rmse:.4f}\")\n",
    "    \n",
    "    # Stage 2: Now train both pathways together\n",
    "    print(\"\\nStage 2: Fine-tuning with implicit feedback...\")\n",
    "    for epoch in range(6, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            user_ids, item_ids, _, _, ratings, implicit_fbk = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            implicit_fbk = implicit_fbk.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get full model output (includes both explicit and implicit paths)\n",
    "            predictions = model(user_ids, item_ids)\n",
    "            \n",
    "            # Calculate explicit rating loss\n",
    "            explicit_loss = explicit_criterion(predictions, ratings)\n",
    "            \n",
    "            # Extract the implicit predictions\n",
    "            # We need to access the tbr_predictor output before sigmoid activation\n",
    "            with torch.no_grad():  # We don't need gradients for this\n",
    "                user_embed = model.tbr_user_embedding(user_ids)\n",
    "                item_embed = model.tbr_item_embedding(item_ids)\n",
    "                tbr_concat = torch.cat([user_embed, item_embed], dim=1)\n",
    "                implicit_preds_raw = model.tbr_predictor(tbr_concat).squeeze()\n",
    "            \n",
    "            # Calculate implicit loss\n",
    "            implicit_loss = implicit_criterion(implicit_preds_raw, implicit_fbk)\n",
    "            \n",
    "            # Combine losses\n",
    "            loss = explicit_loss + implicit_weight * implicit_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "        val_rmse = evaluate_model(model, valid_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} - Val RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_implicit_model.pth')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_implicit_model.pth', weight_only=True))\n",
    "    return model, best_rmse\n",
    "\n",
    "def train_implicit_model_enhanced(model, train_loader, valid_loader, optimizer, device, \n",
    "                         explicit_criterion=nn.MSELoss(),\n",
    "                         implicit_criterion=nn.BCEWithLogitsLoss(),\n",
    "                         implicit_weight=0.2,\n",
    "                         stage1_epochs=20,      # Increased from 5\n",
    "                         stage1_patience=5,     # New parameter\n",
    "                         stage2_epochs=20,      # Total epochs (was 20)\n",
    "                         stage2_patience=5):\n",
    "    \"\"\"\n",
    "    Enhanced training with early stopping for both stages\n",
    "    \"\"\"\n",
    "    # Stage 1: Train only explicit pathway with early stopping\n",
    "    print(\"Stage 1: Training explicit pathway only...\")\n",
    "    \n",
    "    explicit_params = [\n",
    "        model.user_embedding.parameters(),\n",
    "        model.item_embedding.parameters(),\n",
    "        model.user_bias.parameters(), \n",
    "        model.item_bias.parameters(),\n",
    "        model.mlp.parameters(),\n",
    "        model.explicit_predictor.parameters()\n",
    "    ]\n",
    "    explicit_optimizer = torch.optim.Adam(chain(*explicit_params), lr=0.001)\n",
    "    \n",
    "    # Initialize tracking variables for stage 1\n",
    "    best_stage1_rmse = float('inf')\n",
    "    stage1_counter = 0\n",
    "    \n",
    "    for epoch in range(1, stage1_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            user_ids, item_ids, _, _, ratings, _ = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            \n",
    "            explicit_optimizer.zero_grad()\n",
    "            predictions = model(user_ids, item_ids, is_eval=True)\n",
    "            loss = explicit_criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            explicit_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "        val_rmse = evaluate_model(model, valid_loader, device, use_explicit_only=True)\n",
    "        print(f\"Epoch {epoch}/{stage1_epochs} (Explicit) - Val RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_rmse < best_stage1_rmse:\n",
    "            best_stage1_rmse = val_rmse\n",
    "            stage1_counter = 0\n",
    "            # Save best stage 1 model\n",
    "            torch.save(model.state_dict(), 'best_stage1_model.pth')\n",
    "            print(f\"  Stage 1 model improved - saving checkpoint\")\n",
    "        else:\n",
    "            stage1_counter += 1\n",
    "            print(f\"  No improvement for {stage1_counter} epochs\")\n",
    "        \n",
    "        # Early stopping for stage 1\n",
    "        if stage1_counter >= stage1_patience:\n",
    "            print(f\"Early stopping stage 1 after {epoch} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model from stage 1\n",
    "    print(\"Loading best stage 1 model for fine-tuning...\")\n",
    "    model.load_state_dict(torch.load('best_stage1_model.pth', weight_only=True))\n",
    "    \n",
    "    # Initialize tracking variables for stage 2\n",
    "    best_stage2_rmse = float('inf')\n",
    "    stage2_counter = 0\n",
    "    \n",
    "    # Stage 2: Fine-tune with implicit feedback\n",
    "    print(\"\\nStage 2: Fine-tuning with implicit feedback...\")\n",
    "    \n",
    "    # Learning rate scheduler for stage 2\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    for epoch in range(1, stage2_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            user_ids, item_ids, _, _, ratings, implicit_fbk = batch\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            implicit_fbk = implicit_fbk.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Full forward pass\n",
    "            predictions = model(user_ids, item_ids)\n",
    "            \n",
    "            # Calculate explicit rating loss\n",
    "            explicit_loss = explicit_criterion(predictions, ratings)\n",
    "            \n",
    "            # Get implicit predictions for loss calculation\n",
    "            tbr_user = model.tbr_user_embedding(user_ids)\n",
    "            tbr_item = model.tbr_item_embedding(item_ids)\n",
    "            tbr_concat = torch.cat([tbr_user, tbr_item], dim=1)\n",
    "            implicit_preds = model.tbr_predictor(tbr_concat).squeeze()\n",
    "            \n",
    "            # Calculate implicit loss\n",
    "            implicit_loss = implicit_criterion(implicit_preds, implicit_fbk)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = explicit_loss + implicit_weight * implicit_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "        val_rmse = evaluate_model(model, valid_loader, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_rmse)\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{stage2_epochs} - Val RMSE: {val_rmse:.4f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_rmse < best_stage2_rmse:\n",
    "            best_stage2_rmse = val_rmse\n",
    "            stage2_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_implicit_model.pth')\n",
    "            print(f\"  Model improved - saving checkpoint\")\n",
    "        else:\n",
    "            stage2_counter += 1\n",
    "            print(f\"  No improvement for {stage2_counter} epochs\")\n",
    "        \n",
    "        # Early stopping for stage 2\n",
    "        if stage2_counter >= stage2_patience:\n",
    "            print(f\"Early stopping stage 2 after {epoch} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_implicit_model.pth', weight_only=True))\n",
    "    return model, min(best_stage1_rmse, best_stage2_rmse)  # Return the best overall RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7927f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "train_df, valid_df = read_data_df()\n",
    "tbr_data = read_tbr_data()\n",
    "\n",
    "# Prepare implicit feedback data\n",
    "implicit_data = prepare_implicit_feedback(train_df, tbr_data, threshold=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e2dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Determin dimensions\n",
    "num_users = max(train_df['sid'].max(), tbr_data['sid'].max()) + 1\n",
    "num_items = max(train_df['pid'].max(), tbr_data['pid'].max()) + 1\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImplicitFeedbackDataset(\n",
    "    train_df, \n",
    "    implicit_data\n",
    ")\n",
    "\n",
    "valid_dataset = ImplicitFeedbackDataset(\n",
    "    valid_df,\n",
    "    implicit_data\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = ImplicitNeuMF(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    embedding_dim=64,\n",
    "    mlp_dims=[128, 64, 32],\n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "# Setup optimizer and loss functions\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "explicit_criterion = nn.MSELoss()\n",
    "implicit_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e16ebcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Training explicit pathway only...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Explicit) - Val RMSE: 1.1706\n",
      "Epoch 2/5 (Explicit) - Val RMSE: 1.0575\n",
      "Epoch 3/5 (Explicit) - Val RMSE: 0.9925\n",
      "Epoch 4/5 (Explicit) - Val RMSE: 0.9353\n",
      "Epoch 5/5 (Explicit) - Val RMSE: 0.9224\n",
      "\n",
      "Stage 2: Fine-tuning with implicit feedback...\n",
      "Epoch 6/20 - Val RMSE: 0.9113\n",
      "Epoch 7/20 - Val RMSE: 0.9038\n",
      "Epoch 8/20 - Val RMSE: 0.8886\n",
      "Epoch 9/20 - Val RMSE: 0.8810\n",
      "Epoch 10/20 - Val RMSE: 0.8720\n",
      "Epoch 11/20 - Val RMSE: 0.8714\n",
      "Epoch 12/20 - Val RMSE: 0.8706\n",
      "Epoch 13/20 - Val RMSE: 0.8701\n",
      "Epoch 14/20 - Val RMSE: 0.8702\n",
      "Epoch 15/20 - Val RMSE: 0.8706\n",
      "Epoch 16/20 - Val RMSE: 0.8698\n",
      "Epoch 17/20 - Val RMSE: 0.8701\n",
      "Epoch 18/20 - Val RMSE: 0.8699\n",
      "Epoch 19/20 - Val RMSE: 0.8714\n",
      "Epoch 20/20 - Val RMSE: 0.8723\n",
      "Training complete! Best RMSE: 0.8698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55864/745506640.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_implicit_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model, best_rmse = train_implicit_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    explicit_criterion=explicit_criterion,\n",
    "    implicit_criterion=implicit_criterion,\n",
    "    epochs=20,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "print(f\"Training complete! Best RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aeb2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and loss functions\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "explicit_criterion = nn.MSELoss()\n",
    "implicit_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4b1dd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Training explicit pathway only...\n",
      "Epoch 1/20 (Explicit) - Val RMSE: 0.8721\n",
      "  Stage 1 model improved - saving checkpoint\n",
      "Epoch 2/20 (Explicit) - Val RMSE: 0.8714\n",
      "  Stage 1 model improved - saving checkpoint\n",
      "Epoch 3/20 (Explicit) - Val RMSE: 0.8724\n",
      "  No improvement for 1 epochs\n",
      "Epoch 4/20 (Explicit) - Val RMSE: 0.8743\n",
      "  No improvement for 2 epochs\n",
      "Epoch 5/20 (Explicit) - Val RMSE: 0.8731\n",
      "  No improvement for 3 epochs\n",
      "Epoch 6/20 (Explicit) - Val RMSE: 0.8743\n",
      "  No improvement for 4 epochs\n",
      "Epoch 7/20 (Explicit) - Val RMSE: 0.8741\n",
      "  No improvement for 5 epochs\n",
      "Early stopping stage 1 after 7 epochs\n",
      "Loading best stage 1 model for fine-tuning...\n",
      "\n",
      "Stage 2: Fine-tuning with implicit feedback...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55864/224086864.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_stage1_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Val RMSE: 0.8749, LR: 0.001000\n",
      "  Model improved - saving checkpoint\n",
      "Epoch 2/20 - Val RMSE: 0.8732, LR: 0.001000\n",
      "  Model improved - saving checkpoint\n",
      "Epoch 3/20 - Val RMSE: 0.8742, LR: 0.001000\n",
      "  No improvement for 1 epochs\n",
      "Epoch 4/20 - Val RMSE: 0.8758, LR: 0.001000\n",
      "  No improvement for 2 epochs\n",
      "Epoch 5/20 - Val RMSE: 0.8769, LR: 0.000500\n",
      "  No improvement for 3 epochs\n",
      "Epoch 6/20 - Val RMSE: 0.8788, LR: 0.000500\n",
      "  No improvement for 4 epochs\n",
      "Epoch 7/20 - Val RMSE: 0.8787, LR: 0.000500\n",
      "  No improvement for 5 epochs\n",
      "Early stopping stage 2 after 7 epochs\n",
      "Training complete! Best RMSE: 0.8714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55864/224086864.py:345: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_implicit_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "model, best_rmse = train_implicit_model_enhanced(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    explicit_criterion=explicit_criterion,\n",
    "    implicit_criterion=implicit_criterion\n",
    ")\n",
    "\n",
    "print(f\"Training complete! Best RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734963a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
