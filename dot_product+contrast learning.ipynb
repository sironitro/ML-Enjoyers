{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d799b9af-d742-4cac-a937-06102e652812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0063fd35-caac-4ced-b13e-b49cfb58d9a2",
   "metadata": {},
   "source": [
    "Make sure that results are reproducible by using a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73627bd-1106-4276-a498-32b44f1b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b93bc867-b2d9-4cf7-9bb8-ecb13c663eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"C:\\Users\\loris\\OneDrive\\ETH\\Group Project\"\n",
    "\n",
    "\n",
    "def read_data_df():\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    df[\"rating\"] = df[\"rating\"].astype(float)\n",
    "    return train_test_split(df[[\"sid\", \"pid\", \"rating\"]], test_size=0.25, random_state=SEED)\n",
    "\n",
    "\n",
    "def read_data_matrix(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Returns matrix view of the training data, where columns are scientists (sid) and\n",
    "    rows are papers (pid).\"\"\"\n",
    "\n",
    "    return df.pivot(index=\"sid\", columns=\"pid\", values=\"rating\").values\n",
    "\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "\n",
    "def make_submission(pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file that can be submitted to kaggle.\n",
    "\n",
    "    Inputs:\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs a score.\n",
    "        filename: File to save the submission to.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Get sids and pids\n",
    "    sid_pid = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0]\n",
    "    pids = sid_pid[1]\n",
    "    sids = sids.astype(int).values\n",
    "    pids = pids.astype(int).values\n",
    "    \n",
    "    df[\"rating\"] = pred_fn(sids, pids)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def impute_values(mat: np.ndarray) -> np.ndarray:\n",
    "    return np.nan_to_num(mat, nan=3.0)\n",
    "\n",
    "\n",
    "\n",
    "train_df, valid_df = read_data_df()\n",
    "train_mat = read_data_matrix(train_df)\n",
    "train_mat = impute_values(train_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a038331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDpp(nn.Module):\n",
    "    def __init__(self, num_scientists: int = 10000, num_papers: int = 10000, emb_dim: int = 32, s2p: dict = dict(), global_mean: torch.float32 = 3.82):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.s2p = s2p\n",
    "\n",
    "        # embeddings for scientists and papers\n",
    "        self.scientist_factors = nn.Embedding(num_scientists, emb_dim)\n",
    "        self.paper_factors = nn.Embedding(num_papers, emb_dim)\n",
    "        self.scientist_bias = nn.Embedding(num_scientists, 1)\n",
    "        self.paper_bias = nn.Embedding(num_papers, 1)\n",
    "        self.implicit_factors = nn.Embedding(num_papers, emb_dim)\n",
    "\n",
    "        # global average rating - TODO: maybe come up with smth better\n",
    "        self.global_bias = nn.Parameter(torch.tensor([global_mean]), requires_grad=False)\n",
    "\n",
    "        # init weights - TODO: not tuned rn\n",
    "        nn.init.normal_(self.scientist_factors.weight, std=0.1)\n",
    "        nn.init.normal_(self.paper_factors.weight, std=0.1)\n",
    "        nn.init.normal_(self.implicit_factors.weight, std=0.1)\n",
    "        nn.init.constant_(self.scientist_bias.weight, 0.0)\n",
    "        nn.init.constant_(self.paper_bias.weight, 0.0)\n",
    "\n",
    "    def forward(self, scientist_ids, paper_ids):\n",
    "        # latent factors and biases for current batch\n",
    "        scientist_embeddings = self.scientist_factors(scientist_ids)\n",
    "        paper_embeddings = self.paper_factors(paper_ids)\n",
    "        # squeeze to remove extra dim\n",
    "        scientist_biases = self.scientist_bias(scientist_ids).squeeze()\n",
    "        paper_biases = self.paper_bias(paper_ids).squeeze()\n",
    "\n",
    "        papers = [self.s2p.get(k, []) for k in scientist_ids]\n",
    "\n",
    "        implicit_embeds = []\n",
    "        for sp in papers:\n",
    "            if len(sp) > 0:\n",
    "                y_j = self.implicit_factors(torch.tensor(sp, device=scientist_ids.device))\n",
    "                sum_yj = y_j.sum(dim=0)\n",
    "                norm_yj = sum_yj / torch.sqrt(torch.tensor(len(sp), dtype=torch.float, device=scientist_ids.device))\n",
    "            else:\n",
    "                norm_yj = torch.zeros_like(scientist_embeddings[0])\n",
    "            implicit_embeds.append(norm_yj)\n",
    "        y_u = torch.stack(implicit_embeds)\n",
    "\n",
    "        # dot product for interaction\n",
    "        interaction = ((scientist_embeddings + y_u)  * paper_embeddings).sum(dim=1)\n",
    "\n",
    "        # predict ratings\n",
    "        predicted_ratings = interaction + scientist_biases + paper_biases + self.global_bias\n",
    "        return predicted_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67fcd1d8-93dc-45e7-9706-554b9d1edc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86c8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW\n",
    "def get_embeddings(model, sid, pid):\n",
    "    return model.scientist_emb(sid), model.paper_emb(pid)\n",
    "\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.triplets = self._generate_triplets()\n",
    "\n",
    "    def _generate_triplets(self):\n",
    "        triplets = []\n",
    "        grouped = self.df.groupby(\"sid\")\n",
    "        for sid, group in grouped:\n",
    "            pos = group[group[\"rating\"] >= 4][\"pid\"].values\n",
    "            neg = group[group[\"rating\"] <= 2][\"pid\"].values\n",
    "            if len(pos) > 0 and len(neg) > 0:\n",
    "                for p in pos:\n",
    "                    for n in neg:\n",
    "                        triplets.append((sid, p, n))\n",
    "        return triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid, pid_pos, pid_neg = self.triplets[idx]\n",
    "        return torch.tensor(sid), torch.tensor(pid_pos), torch.tensor(pid_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eaf96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW\n",
    "triplet_dataset = TripletDataset(train_df)\n",
    "triplet_loader = torch.utils.data.DataLoader(triplet_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0dbf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c1d236-0b95-4a68-9447-4ad7042d5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDotProductModel(nn.Module):\n",
    "    def __init__(self, num_scientists: int, num_papers: int, dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assign to each scientist and paper an embedding\n",
    "        self.scientist_emb = nn.Embedding(num_scientists, dim)\n",
    "        self.paper_emb = nn.Embedding(num_papers, dim)\n",
    "        \n",
    "    def forward(self, sid: torch.Tensor, pid: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            sid: [B,], int\n",
    "            pid: [B,], int\n",
    "        \n",
    "        Outputs: [B,], float\n",
    "        \"\"\"\n",
    "\n",
    "        # Per-pair dot product\n",
    "        return torch.sum(self.scientist_emb(sid) * self.paper_emb(pid), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135658a9-8890-4e1e-a5ae-a500e7fc0da2",
   "metadata": {},
   "source": [
    "Set $d=32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5637bee3-2b0c-425e-b6b1-e13547d5ade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model (10k scientists, 1k papers, 32-dimensional embeddings) and optimizer\n",
    "model = EmbeddingDotProductModel(10_000, 1_000, 32).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e866a9d-d5c6-40f1-b27f-d934eb6a5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df: pd.DataFrame) -> torch.utils.data.Dataset:\n",
    "    \"\"\"Conversion from pandas data frame to torch dataset.\"\"\"\n",
    "    \n",
    "    sids = torch.from_numpy(df[\"sid\"].to_numpy())\n",
    "    pids = torch.from_numpy(df[\"pid\"].to_numpy())\n",
    "    ratings = torch.from_numpy(df[\"rating\"].to_numpy()).float()\n",
    "    return torch.utils.data.TensorDataset(sids, pids, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "636474d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wishlist data\n",
    "wishlist_df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))\n",
    "\n",
    "# No need to split any columns â€” they're already 'sid' and 'pid'\n",
    "wishlist_df[\"sid\"] = wishlist_df[\"sid\"].astype(int)\n",
    "wishlist_df[\"pid\"] = wishlist_df[\"pid\"].astype(int)\n",
    "\n",
    "# Assign soft rating\n",
    "wishlist_df[\"rating\"] = 3.5\n",
    "\n",
    "# Load and clean the original ratings\n",
    "train_df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "train_df[[\"sid\", \"pid\"]] = train_df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "train_df[\"sid\"] = train_df[\"sid\"].astype(int)\n",
    "train_df[\"pid\"] = train_df[\"pid\"].astype(int)\n",
    "train_df[\"rating\"] = train_df[\"rating\"].astype(float)\n",
    "\n",
    "# Combine both into one training set\n",
    "augmented_train_df = pd.concat([train_df[[\"sid\", \"pid\", \"rating\"]], wishlist_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e05764b4-5103-40fa-8910-3d84ea28a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(train_df)\n",
    "valid_dataset = get_dataset(valid_df)\n",
    "train_loader = torch.utils.data.DataLoader(get_dataset(augmented_train_df), batch_size=64, shuffle=True)\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8240462-5fc0-4c51-8e84-1082da8bf295",
   "metadata": {},
   "source": [
    "Training loop, which we run for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train RMSE: 5.268\n",
      "[Epoch 2] Train RMSE: 1.935\n",
      "[Epoch 3] Train RMSE: 0.872\n",
      "[Epoch 4] Train RMSE: 0.854\n",
      "[Epoch 5] Train RMSE: 0.842\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(get_dataset(augmented_train_df), batch_size=64, shuffle=True)\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    for sid, pid, rating in train_loader:\n",
    "        sid, pid, rating = sid.to(device), pid.to(device), rating.to(device)\n",
    "        pred = model(sid, pid)\n",
    "        loss = F.mse_loss(pred, rating)\n",
    "            # --- Contrastive loss step ---\n",
    "    for sid, pid_pos, pid_neg in triplet_loader:\n",
    "        sid = sid.to(device)\n",
    "        pid_pos = pid_pos.to(device)\n",
    "        pid_neg = pid_neg.to(device)\n",
    "\n",
    "        anchor = model.scientist_emb(sid)\n",
    "        positive = model.paper_emb(pid_pos)\n",
    "        negative = model.paper_emb(pid_neg)\n",
    "\n",
    "        contrastive = triplet_loss_fn(anchor, positive, negative)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        contrastive.backward()\n",
    "        optim.step()\n",
    "\n",
    "        total_loss += loss.item() * len(sid)\n",
    "        total += len(sid)\n",
    "    print(f\"[Epoch {epoch+1}] Train RMSE: {(total_loss / total)**0.5:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb2f5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 0.891\n"
     ]
    }
   ],
   "source": [
    "#NEW\n",
    "model.eval()\n",
    "def pred_fn(sids, pids):\n",
    "    with torch.no_grad():\n",
    "        sids = torch.tensor(sids).to(device)\n",
    "        pids = torch.tensor(pids).to(device)\n",
    "        return model(sids, pids).clamp(1, 5).cpu().numpy()\n",
    "\n",
    "val_rmse = evaluate(valid_df, pred_fn)\n",
    "print(f\"Validation RMSE: {val_rmse:.3f}\")\n",
    "\n",
    "make_submission(pred_fn, \"dot_product_contrastive_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f051b-6854-4fc4-a380-2f6951768ee4",
   "metadata": {},
   "source": [
    "As we can see, this method already provides an improvement on the validation dataset over the naive SVD method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73bec67-2ee0-4683-beb1-a102dac072d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fn = lambda sids, pids: model(torch.from_numpy(sids).to(device), torch.from_numpy(pids).to(device)).clamp(1, 5).cpu().numpy()\n",
    "\n",
    "# Evaluate on validation data\n",
    "with torch.no_grad():\n",
    "    val_score = evaluate(valid_df, pred_fn)\n",
    "\n",
    "print(f\"Validation RMSE: {val_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42b995-2e08-4cfc-90aa-1b59a7e9fd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    make_submission(pred_fn, \"learned_embedding_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fdd37c-3c98-497e-881b-29ce59f88d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
