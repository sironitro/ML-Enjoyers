{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR, StepLR, CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Callable, Tuple\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, KFold\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff981f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sids = df['sid'].values.astype(np.int64)\n",
    "        self.pids = df['pid'].values.astype(np.int64)\n",
    "        self.ratings = df['rating'].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sids[idx], self.pids[idx], self.ratings[idx]\n",
    "    \n",
    "class EnhancedNeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim=64, mlp_layer_sizes=[128,64,32], dropout=0.3, activation_fn=nn.LeakyReLU(), leaky_relu_slope=0.1):\n",
    "        super().__init__()\n",
    "        # GMF embeddings\n",
    "        self.user_gmf = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_gmf = nn.Embedding(num_items, mf_dim)\n",
    "        # MLP embeddings\n",
    "        self.user_mlp = nn.Embedding(num_users, mlp_layer_sizes[0])\n",
    "        self.item_mlp = nn.Embedding(num_items, mlp_layer_sizes[0])\n",
    "\n",
    "        # Add user and item bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "\n",
    "        # Improved MLP with BatchNorm\n",
    "        mlp_layers = []\n",
    "        in_size = mlp_layer_sizes[0] * 2\n",
    "        for out_size in mlp_layer_sizes[1:]:\n",
    "          if activation_fn == nn.LeakyReLU:\n",
    "            activation = activation_fn(negative_slope=leaky_relu_slope)\n",
    "          else:\n",
    "            activation = activation_fn()\n",
    "          mlp_layers += [\n",
    "            nn.Linear(in_size, out_size),\n",
    "            nn.BatchNorm1d(out_size),\n",
    "            activation_fn(),\n",
    "            nn.Dropout(dropout)\n",
    "          ]\n",
    "          in_size = out_size\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "\n",
    "        # Final prediction\n",
    "        self.predict = nn.Linear(mf_dim + mlp_layer_sizes[-1], 1)\n",
    "\n",
    "        # Initialization\n",
    "        nn.init.normal_(self.user_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_mlp.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.kaiming_uniform_(self.predict.weight, a=1, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        # GMF path\n",
    "        gmf = self.user_gmf(u) * self.item_gmf(i)\n",
    "        # MLP path\n",
    "        mlp = self.mlp(torch.cat([self.user_mlp(u), self.item_mlp(i)], dim=1))\n",
    "        # Combine paths\n",
    "        x = torch.cat([gmf, mlp], dim=1)\n",
    "        base_pred = self.predict(x).squeeze()\n",
    "        # Add bias terms\n",
    "        u_bias = self.user_bias(u).squeeze()\n",
    "        i_bias = self.item_bias(i).squeeze()\n",
    "        return base_pred + u_bias + i_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "\n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25, random_state=SEED, stratify=df[\"sid\"])\n",
    "    return train_df, valid_df\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "\n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "def tune_enhanced_neumf_cv(\n",
    "    df_norm,               # full normalized ratings DataFrame\n",
    "    num_users,             # total # of users\n",
    "    num_items,             # total # of items\n",
    "    device,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"5-fold CV grid search over mf_dim, layer sizes, dropout, weight decay, and lr.\"\"\"\n",
    "    # 1) Hyperparameters to tune\n",
    "    param_grid = {\n",
    "        'mf_dim':           [16, 32, 64, 128],\n",
    "        'mlp_layer_sizes': [[64,32,16], [128,64,32], [128,64,32,16], [256,128,64]],\n",
    "        'dropout':         [0.1, 0.2, 0.3, 0.4],\n",
    "        'weight_decay':    [1e-4, 5e-3, 1e-3],\n",
    "        'lr':              [1e-3, 5e-4, 1e-4],\n",
    "    }\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    best_avg_rmse = float('inf')\n",
    "    best_params   = None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        fold_rmses = []\n",
    "        print(f\"Evaluating params: {params}\")\n",
    "\n",
    "        # cross-val loop\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(df_norm), 1):\n",
    "            train_df, val_df = df_norm.iloc[train_idx], df_norm.iloc[val_idx]\n",
    "\n",
    "            # rebuild loader each fold\n",
    "            train_loader = DataLoader(RatingDataset(train_df), batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "            # fresh model per fold\n",
    "            model = EnhancedNeuMF(\n",
    "                num_users=num_users,\n",
    "                num_items=num_items,\n",
    "                mf_dim=params['mf_dim'],\n",
    "                mlp_layer_sizes=params['mlp_layer_sizes'],\n",
    "                dropout=params['dropout'],\n",
    "                activation_fn=nn.LeakyReLU,          # fixed\n",
    "                leaky_relu_slope=0.2            # default for non-leaky or ReLU\n",
    "            ).to(device)\n",
    "\n",
    "            # optimizer with tuned lr + weight_decay\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=params['lr'],\n",
    "                weight_decay=params['weight_decay']\n",
    "            )\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # fixed scheduler\n",
    "            scheduler = ReduceLROnPlateau\n",
    "\n",
    "            # train + eval\n",
    "            rmse = train_enhanced(\n",
    "                model=model,\n",
    "                train_df=train_df,\n",
    "                valid_df=val_df,\n",
    "                loader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                device=device,\n",
    "                scheduler=scheduler,\n",
    "                scheduler_params={'mode': 'min', 'factor': 0.5, 'patience': 3, 'min_lr': 1e-6},\n",
    "                epochs=20,\n",
    "                patience=5\n",
    "            )\n",
    "            fold_rmses.append(rmse)\n",
    "            print(f\"  Fold {fold_idx} RMSE: {rmse:.4f}\")\n",
    "\n",
    "        avg_rmse = sum(fold_rmses) / len(fold_rmses)\n",
    "        print(f\" â†’ Avg CV RMSE: {avg_rmse:.4f}\\n\")\n",
    "\n",
    "        if avg_rmse < best_avg_rmse:\n",
    "            best_avg_rmse = avg_rmse\n",
    "            best_params   = params\n",
    "\n",
    "    print(f\"Best 5-fold CV RMSE: {best_avg_rmse:.4f}\")\n",
    "    print(f\"Best params: {best_params}\")\n",
    "    return best_params\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Apply user-specific normalization to ratings\"\"\"\n",
    "    # Get global mean\n",
    "    global_mean = df['rating'].mean()\n",
    "    print(f\"Global mean rating: {global_mean:.4f}\")\n",
    "\n",
    "    # Get user biases (average rating deviation from global mean)\n",
    "    user_biases = df.groupby('sid')['rating'].mean() - global_mean\n",
    "\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_norm = df.copy()\n",
    "\n",
    "    # Normalize ratings by user bias\n",
    "    def normalize_rating(row):\n",
    "        user_id = row['sid']\n",
    "        return row['rating'] - user_biases.get(user_id, 0)\n",
    "\n",
    "    df_norm['rating'] = df_norm.apply(normalize_rating, axis=1)\n",
    "\n",
    "    return df_norm, user_biases, global_mean\n",
    "\n",
    "def train_enhanced(model, train_df, valid_df, loader, optimizer, criterion, device,\n",
    "                  scheduler, scheduler_params, epochs=20, patience=5, clip_norm=1.0):\n",
    "    best_rmse = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler_instance = scheduler(optimizer, **scheduler_params)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training step\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for sids, pids, ratings in loader:\n",
    "            sids, pids, ratings = sids.to(device), pids.to(device), ratings.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(sids, pids)\n",
    "            loss = criterion(preds, ratings)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(ratings)\n",
    "\n",
    "        # Create prediction function for evaluation\n",
    "        def pred_fn(s, p):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(\n",
    "                    torch.from_numpy(s).to(device),\n",
    "                    torch.from_numpy(p).to(device)\n",
    "                ).detach().cpu().numpy()\n",
    "            return np.clip(preds, 1, 5)\n",
    "\n",
    "        # Evaluate on both train and validation sets\n",
    "        train_rmse = evaluate(train_df, pred_fn)\n",
    "        valid_rmse = evaluate(valid_df, pred_fn)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if isinstance(scheduler_instance, torch.optim.lr_scheduler.CyclicLR):\n",
    "          scheduler_instance.step()  # Call step() without arguments for CyclicLR\n",
    "        else:\n",
    "          scheduler_instance.step(valid_rmse)  # Call step() with validation loss for other schedulers\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} â€” Train RMSE: {train_rmse:.4f}, Valid RMSE: {valid_rmse:.4f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if valid_rmse < best_rmse:\n",
    "            best_rmse = valid_rmse\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_ncf.pth')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nBest Val RMSE: {best_rmse:.4f}\")\n",
    "    return best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac98c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "train_df_norm, user_biases, global_mean = preprocess_data(train_df)\n",
    "valid_df_norm = valid_df.copy()\n",
    "valid_df_norm['rating'] = valid_df_norm.apply(\n",
    "    lambda row: row['rating'] - user_biases.get(row['sid'], 0),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Determine number of users and items\n",
    "num_users = train_df['sid'].max() + 1\n",
    "num_items = train_df['pid'].max() + 1\n",
    "print(f\"Num users: {num_users}, Num items: {num_items}\")\n",
    "\n",
    "# Prepare data loader with normalized data\n",
    "train_loader = DataLoader(\n",
    "    RatingDataset(train_df_norm),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a5de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = tune_enhanced_neumf_cv(train_df_norm, num_users, num_items, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a936143",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = EnhancedNeuMF(\n",
    "    num_users, num_items,\n",
    "    mf_dim=best_params['mf_dim'],\n",
    "    mlp_layer_sizes=best_params['mlp_layer_sizes'],\n",
    "    dropout=best_params['dropout'],\n",
    "    activation_fn=params['activation_fn']\n",
    ").to(device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
