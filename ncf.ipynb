{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efbf5319",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93393cb7",
   "metadata": {},
   "source": [
    "## Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc595091",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b96db5",
   "metadata": {},
   "source": [
    "# 1. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_DIR = \"/cluster/courses/cil/collaborative_filtering/data\"\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25, random_state=SEED, stratify=df[\"sid\"])\n",
    "    return train_df, valid_df\n",
    "\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "\n",
    "def make_submission(pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray], filename: os.PathLike):\n",
    "    \"\"\"Makes a submission CSV file that can be submitted to kaggle.\n",
    "\n",
    "    Inputs:\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs a score.\n",
    "        filename: File to save the submission to.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Get sids and pids\n",
    "    sid_pid = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    sids = sid_pid[0]\n",
    "    pids = sid_pid[1]\n",
    "    sids = sids.astype(int).values\n",
    "    pids = pids.astype(int).values\n",
    "    \n",
    "    df[\"rating\"] = pred_fn(sids, pids)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc725ad",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b0e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffcd4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global rating mean: 3.8174\n"
     ]
    }
   ],
   "source": [
    "num_users = train_df['sid'].max() + 1\n",
    "num_items = train_df['pid'].max() + 1\n",
    "\n",
    "global_mean = train_df['rating'].mean()\n",
    "train_df['rating'] = train_df['rating'] - global_mean  # Center the ratings\n",
    "valid_df['rating'] = valid_df['rating'] - global_mean  # Center validation ratings too\n",
    "\n",
    "print(f\"Global rating mean: {global_mean:.4f}\")\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    # Add global mean to ratings for comparison with predictions\n",
    "    actual_ratings = valid_df[\"rating\"].values + global_mean\n",
    "    return root_mean_squared_error(actual_ratings, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ee273",
   "metadata": {},
   "source": [
    "# 3. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbb4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatingsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users   = torch.LongTensor(df['sid'].values)\n",
    "        self.items   = torch.LongTensor(df['pid'].values)\n",
    "        self.ratings = torch.FloatTensor(df['rating'].values)\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "train_ds = RatingsDataset(train_df)\n",
    "valid_ds = RatingsDataset(valid_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b158fcb",
   "metadata": {},
   "source": [
    "# 5. NCF Model (MLP-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52348997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=32, mlp_layers=[64,32,16], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        \n",
    "        mlp_input = emb_size * 2\n",
    "        layers = []\n",
    "        for units in mlp_layers:\n",
    "            layers += [\n",
    "                nn.Linear(mlp_input, units),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            mlp_input = units\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(mlp_input, 1)\n",
    "        \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u = self.user_emb(user_idx)\n",
    "        v = self.item_emb(item_idx)\n",
    "        x = torch.cat([u, v], dim=-1)\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8b9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        emb_size: int = 32,\n",
    "        mlp_layers: list = [64, 32, 16],\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # embeddings\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        # bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "\n",
    "        # MLP tower with BatchNorm\n",
    "        layers = []\n",
    "        input_dim = emb_size * 2\n",
    "        for units in mlp_layers:\n",
    "            layers += [\n",
    "                nn.Linear(input_dim, units),\n",
    "                nn.BatchNorm1d(units),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            input_dim = units\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # final output\n",
    "        self.out = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u = self.user_emb(user_idx)\n",
    "        v = self.item_emb(item_idx)\n",
    "        x = torch.cat([u, v], dim=1)\n",
    "        x = self.mlp(x)\n",
    "        base = self.out(x).squeeze()\n",
    "        # bias terms\n",
    "        b_u = self.user_bias(user_idx).squeeze()\n",
    "        b_i = self.item_bias(item_idx).squeeze()\n",
    "        return base + b_u + b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6173e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMF tower\n",
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_size=32):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users, emb_size)\n",
    "        self.item_emb = nn.Embedding(num_items, emb_size)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        self.out = nn.Linear(emb_size, 1)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        u = self.user_emb(user_idx)\n",
    "        v = self.item_emb(item_idx)\n",
    "        x = u * v                       # element-wise product\n",
    "        base = self.out(x).squeeze()\n",
    "        b_u = self.user_bias(user_idx).squeeze()\n",
    "        b_i = self.item_bias(item_idx).squeeze()\n",
    "        return base + b_u + b_i\n",
    "\n",
    "# Fusion by weighted sum of predictions\n",
    "class FusedNeuMF(nn.Module):\n",
    "    def __init__(self, gmf: GMF, mlp: NCF2, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.gmf = gmf\n",
    "        self.mlp = mlp\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        pred_gmf = self.gmf(user_idx, item_idx)\n",
    "        pred_mlp = self.mlp(user_idx, item_idx)\n",
    "        return self.alpha * pred_gmf + (1 - self.alpha) * pred_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c7f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full NeuMF with concat-fusion\n",
    "class NeuMFConcat(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        mf_emb_size: int = 32,\n",
    "        mlp_emb_size: int = 32,\n",
    "        mlp_layers: list = [64, 32, 16],\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # — MF (GMF) embeddings —\n",
    "        self.user_emb_mf = nn.Embedding(num_users, mf_emb_size)\n",
    "        self.item_emb_mf = nn.Embedding(num_items, mf_emb_size)\n",
    "        # — MLP embeddings (can be same size or different) —\n",
    "        self.user_emb_mlp = nn.Embedding(num_users, mlp_emb_size)\n",
    "        self.item_emb_mlp = nn.Embedding(num_items, mlp_emb_size)\n",
    "\n",
    "        # — MLP tower (over concatenated mlp_emb) —\n",
    "        layers = []\n",
    "        in_dim = mlp_emb_size * 2\n",
    "        for h in mlp_layers:\n",
    "            layers += [\n",
    "                nn.Linear(in_dim, h),\n",
    "                nn.BatchNorm1d(h),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            in_dim = h\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "        # — Final fusion layer —\n",
    "        # input dim = mf_emb_size + last-mlp-layer\n",
    "        self.fusion = nn.Linear(mf_emb_size + in_dim, 1)\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        # MF path\n",
    "        u_mf = self.user_emb_mf(u)\n",
    "        i_mf = self.item_emb_mf(i)\n",
    "        mf_vec = u_mf * i_mf                  # elementwise\n",
    "\n",
    "        # MLP path\n",
    "        u_ml = self.user_emb_mlp(u)\n",
    "        i_ml = self.item_emb_mlp(i)\n",
    "        mlp_vec = torch.cat([u_ml, i_ml], dim=1)\n",
    "        mlp_vec = self.mlp(mlp_vec)\n",
    "\n",
    "        # fuse\n",
    "        z = torch.cat([mf_vec, mlp_vec], dim=1)\n",
    "        return self.fusion(z).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04d2e9",
   "metadata": {},
   "source": [
    "# 6. Setup device, model, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a38f2f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# instantiate towers\n",
    "gmf_model = GMF(num_users, num_items, emb_size=32).to(device)\n",
    "mlp_model = NCF2(num_users, num_items, emb_size=32).to(device)\n",
    "\n",
    "# fused model\n",
    "model = FusedNeuMF(gmf_model, mlp_model, alpha=0.5).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f833ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuMFConcat(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    mf_emb_size=32,\n",
    "    mlp_emb_size=32,\n",
    "    mlp_layers=[64,32,16],\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d05a36",
   "metadata": {},
   "source": [
    "# 7. Training loop with validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e50cd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_fn(sids: np.ndarray, pids: np.ndarray) -> np.ndarray:\n",
    "    model.eval()\n",
    "    u_idx = torch.LongTensor(sids).to(device)\n",
    "    i_idx = torch.LongTensor(pids).to(device)\n",
    "    with torch.no_grad():\n",
    "        raw_pred = model(u_idx, i_idx).cpu().numpy()\n",
    "    # add global mean back\n",
    "    raw_pred += global_mean\n",
    "    # clamp to valid range\n",
    "    return np.clip(raw_pred, train_df[\"rating\"].min(), train_df[\"rating\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d673395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — Train RMSE: 0.9074, Valid RMSE: 0.9148, LR: 0.001000\n",
      "Epoch 02 — Train RMSE: 0.8852, Valid RMSE: 0.8961, LR: 0.001000\n",
      "Epoch 03 — Train RMSE: 0.8824, Valid RMSE: 0.8946, LR: 0.001000\n",
      "Epoch 04 — Train RMSE: 0.8498, Valid RMSE: 0.8774, LR: 0.001000\n",
      "Epoch 05 — Train RMSE: 0.8344, Valid RMSE: 0.8733, LR: 0.001000\n",
      "Epoch 06 — Train RMSE: 0.8180, Valid RMSE: 0.8701, LR: 0.001000\n",
      "Epoch 07 — Train RMSE: 0.8103, Valid RMSE: 0.8714, LR: 0.001000\n",
      "Epoch 08 — Train RMSE: 0.8029, Valid RMSE: 0.8714, LR: 0.001000\n",
      "Epoch 09 — Train RMSE: 0.7940, Valid RMSE: 0.8723, LR: 0.001000\n",
      "Epoch 10 — Train RMSE: 0.7860, Valid RMSE: 0.8742, LR: 0.000100\n",
      "Epoch 11 — Train RMSE: 0.7745, Valid RMSE: 0.8790, LR: 0.000100\n",
      "Epoch 12 — Train RMSE: 0.7697, Valid RMSE: 0.8812, LR: 0.000100\n",
      "Epoch 13 — Train RMSE: 0.7674, Valid RMSE: 0.8819, LR: 0.000100\n",
      "Epoch 14 — Train RMSE: 0.7638, Valid RMSE: 0.8835, LR: 0.000010\n",
      "Epoch 15 — Train RMSE: 0.7622, Valid RMSE: 0.8848, LR: 0.000010\n",
      "Epoch 16 — Train RMSE: 0.7635, Valid RMSE: 0.8843, LR: 0.000010\n",
      "\n",
      "Early stopping triggered after 16 epochs\n"
     ]
    }
   ],
   "source": [
    "best_valid_rmse = float('inf')\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "epochs_without_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(1, 51): \n",
    "    # Training step\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for u, i, r in train_loader:\n",
    "        u, i, r = u.to(device), i.to(device), r.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(u, i)\n",
    "        loss = criterion(pred, r)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    \n",
    "    # Evaluation\n",
    "    train_rmse = evaluate(train_df, pred_fn)\n",
    "    valid_rmse = evaluate(valid_df, pred_fn)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(valid_rmse)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if valid_rmse < best_valid_rmse:\n",
    "        best_valid_rmse = valid_rmse\n",
    "        epochs_without_improve = 0\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        epochs_without_improve += 1\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} — \"\n",
    "        f\"Train RMSE: {train_rmse:.4f}, \"\n",
    "        f\"Valid RMSE: {valid_rmse:.4f}, \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_without_improve >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch} epochs\")\n",
    "        break\n",
    "\n",
    "# Restore best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d8076",
   "metadata": {},
   "source": [
    "# 8. RMSE valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "651f374c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation RMSE: 0.8859\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluate(valid_df, pred_fn)\n",
    "print(f\"\\nValidation RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a062f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
