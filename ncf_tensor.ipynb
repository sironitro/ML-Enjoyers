{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc7a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3045f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\" \n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "\n",
    "    # You may also want to cast rating to float32\n",
    "    df[\"rating\"] = df[\"rating\"].astype(\"float32\")\n",
    "\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "    return train_df, valid_df\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "def df_to_tf_dataset(df: pd.DataFrame,\n",
    "                     batch_size: int = 256,\n",
    "                     shuffle: bool = True) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame with columns ['sid', 'pid', 'rating'] into\n",
    "    a tf.data.Dataset yielding (features_dict, label) tuples.\n",
    "    \"\"\"\n",
    "    # 1) Pull out NumPy arrays\n",
    "    sid_array = df[\"sid\"].values\n",
    "    pid_array = df[\"pid\"].values\n",
    "    rating_array = df[\"rating\"].values\n",
    "\n",
    "    # 2) Build the dataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            \"sid\": sid_array,\n",
    "            \"pid\": pid_array,\n",
    "        },\n",
    "        rating_array\n",
    "    ))  # :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "    # 3) Shuffle / batch / prefetch\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def get_tf_datasets(batch_size: int = 256) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Reads CSV, splits into train/validation DataFrames,\n",
    "    and returns two tf.data.Datasets.\n",
    "    \"\"\"\n",
    "    train_df, valid_df = read_data_df()\n",
    "    train_ds = df_to_tf_dataset(train_df, batch_size=batch_size, shuffle=True)\n",
    "    val_ds   = df_to_tf_dataset(valid_df, batch_size=batch_size, shuffle=False)\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527c534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = get_tf_datasets(batch_size=1024)\n",
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a030fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_neumf_base_model(user_input, item_input, params):\n",
    "    \"\"\"Builds the NeuMF (GMF + MLP) base model.\n",
    "\n",
    "    Args:\n",
    "      user_input:  Keras Input(shape=(1,), dtype=tf.int32) for user IDs\n",
    "      item_input:  Keras Input(shape=(1,), dtype=tf.int32) for item IDs\n",
    "      params:      dict containing\n",
    "        - num_users:       int, total number of users\n",
    "        - num_items:       int, total number of items\n",
    "        - mf_dim:          int, embedding size for MF branch\n",
    "        - model_layers:    list of ints, layer sizes for the MLP branch\n",
    "        - mf_regularization: float, L2 reg for MF embeddings\n",
    "        - mlp_reg_layers:    list of floats, L2 regs for each MLP layer\n",
    "\n",
    "    Returns:\n",
    "      A tf.keras.Model whose output is the (unnormalized) logit for each (user, item).\n",
    "    \"\"\"\n",
    "    num_users        = params[\"num_users\"]\n",
    "    num_items        = params[\"num_items\"]\n",
    "    mf_dim           = params[\"mf_dim\"]\n",
    "    model_layers     = params[\"model_layers\"]\n",
    "    mf_reg           = params[\"mf_regularization\"]\n",
    "    mlp_reg_layers   = params[\"mlp_reg_layers\"]\n",
    "\n",
    "    # Combined embedding size = MF dim + half of first MLP layer\n",
    "    embedding_size = mf_dim + model_layers[0] // 2\n",
    "    embed_init     = \"glorot_uniform\"\n",
    "\n",
    "    def mf_slice(x):\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "        return x[:, :mf_dim]\n",
    "\n",
    "    def mlp_slice(x):\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "        return x[:, mf_dim:]\n",
    "\n",
    "    # shared user/item embedding tables\n",
    "    user_emb = tf.keras.layers.Embedding(\n",
    "        input_dim=num_users,\n",
    "        output_dim=embedding_size,\n",
    "        embeddings_initializer=embed_init,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(mf_reg),\n",
    "        input_length=1,\n",
    "        name=\"embedding_user\"\n",
    "    )(user_input)\n",
    "\n",
    "    item_emb = tf.keras.layers.Embedding(\n",
    "        input_dim=num_items,\n",
    "        output_dim=embedding_size,\n",
    "        embeddings_initializer=embed_init,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(mf_reg),\n",
    "        input_length=1,\n",
    "        name=\"embedding_item\"\n",
    "    )(item_input)\n",
    "\n",
    "    # MF branch: slice out the first mf_dim components and multiply\n",
    "    mf_user = tf.keras.layers.Lambda(mf_slice, name=\"emb_user_mf\")(user_emb)\n",
    "    mf_item = tf.keras.layers.Lambda(mf_slice, name=\"emb_item_mf\")(item_emb)\n",
    "    mf_vector = tf.keras.layers.Multiply()([mf_user, mf_item])\n",
    "\n",
    "    # MLP branch: slice out the remaining components and feed through dense layers\n",
    "    mlp_user  = tf.keras.layers.Lambda(mlp_slice, name=\"emb_user_mlp\")(user_emb)\n",
    "    mlp_item  = tf.keras.layers.Lambda(mlp_slice, name=\"emb_item_mlp\")(item_emb)\n",
    "    mlp_vector = tf.keras.layers.Concatenate()( [mlp_user, mlp_item] )\n",
    "\n",
    "    # build MLP layers\n",
    "    for idx in range(1, len(model_layers)):\n",
    "        mlp_vector = tf.keras.layers.Dense(\n",
    "            units=model_layers[idx],\n",
    "            activation=\"relu\",\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(mlp_reg_layers[idx])\n",
    "        )(mlp_vector)\n",
    "\n",
    "    # fuse MF + MLP\n",
    "    predict_vector = tf.keras.layers.Concatenate()([mf_vector, mlp_vector])\n",
    "\n",
    "    # final logit\n",
    "    logits = tf.keras.layers.Dense(\n",
    "        units=1,\n",
    "        activation=None,\n",
    "        kernel_initializer=\"lecun_uniform\",\n",
    "        name=\"logits\"\n",
    "    )(predict_vector)\n",
    "\n",
    "    return tf.keras.Model(inputs=[user_input, item_input], outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82392431",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"No-op placeholder for the sampled-softmax metric layer.\"\"\"\n",
    "    def __init__(self, params=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, inputs):\n",
    "        # original pipeline did: [softmax_logits, dup_mask] → mask/metrics\n",
    "        # here we just pass straight through the logits\n",
    "        return inputs[0]\n",
    "\n",
    "class LossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, batch_size=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        softmax_logits, labels, valid_mask = inputs\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        mask   = tf.cast(valid_mask, tf.float32)\n",
    "\n",
    "        # If logits come in shape (batch,2), pick the \"1\" column;\n",
    "        # otherwise assume (batch,1) and squeeze:\n",
    "        if softmax_logits.shape[-1] == 2:\n",
    "            preds = softmax_logits[:, 1]\n",
    "        else:\n",
    "            preds = tf.squeeze(softmax_logits, axis=-1)\n",
    "\n",
    "        # masked MSE\n",
    "        loss = tf.reduce_sum(mask * tf.square(labels - preds)) / tf.reduce_sum(mask)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # return (batch,1) so Keras sees a y_pred\n",
    "        return tf.expand_dims(preds, axis=-1)\n",
    "\n",
    "\n",
    "def build_ncf_keras_model(params):\n",
    "    # 1) define all the Keras inputs\n",
    "    user_input       = tf.keras.layers.Input(shape=(1,), name=\"sid\", dtype=tf.int32)\n",
    "    item_input       = tf.keras.layers.Input(shape=(1,), name=\"pid\", dtype=tf.int32)\n",
    "    valid_pt_mask    = tf.keras.layers.Input(shape=(1,), name=\"valid_pt_mask\", dtype=tf.bool)\n",
    "    dup_mask         = tf.keras.layers.Input(shape=(1,), name=\"duplicate_mask\", dtype=tf.int32)\n",
    "    label_input      = tf.keras.layers.Input(shape=(1,), name=\"train_label\", dtype=tf.bool)\n",
    "\n",
    "    # 2) base NeuMF to get raw logits\n",
    "    base_model = construct_neumf_base_model(user_input, item_input, params)\n",
    "    logits     = base_model.output              # shape=(batch, 1)\n",
    "\n",
    "    # 3) zero-pad + concat so we can later pick the 'positive' logit via gather\n",
    "    zeros         = tf.keras.layers.Lambda(lambda x: x * 0)(logits)\n",
    "    softmax_logits = tf.keras.layers.Concatenate(axis=-1)([zeros, logits])\n",
    "\n",
    "    # 4) hook in the MetricLayer / LossLayer from the official repo\n",
    "    #    (they compute hit-rate, ndcg, and the sampled‐softmax loss under the hood).\n",
    "    if not params[\"keras_use_ctl\"]:\n",
    "        softmax_logits = MetricLayer(params)([softmax_logits, dup_mask])\n",
    "    final_out = LossLayer(params[\"batch_size\"])([softmax_logits, label_input, valid_pt_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs={\n",
    "            \"sid\": user_input,\n",
    "            \"pid\": item_input,\n",
    "            \"valid_pt_mask\": valid_pt_mask,\n",
    "            \"duplicate_mask\": dup_mask,\n",
    "            \"train_label\": label_input\n",
    "        },\n",
    "        outputs=final_out\n",
    "    )\n",
    "\n",
    "def build_ncf_regression_model(params):\n",
    "    sid_in = tf.keras.layers.Input(shape=(1,), name=\"sid\", dtype=tf.int32)\n",
    "    pid_in = tf.keras.layers.Input(shape=(1,), name=\"pid\", dtype=tf.int32)\n",
    "\n",
    "    # reuse your NeuMF body\n",
    "    base = construct_neumf_base_model(sid_in, pid_in, params)\n",
    "    preds = base.output  # shape=(batch,1)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[sid_in, pid_in], outputs=preds)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=\"mse\",\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80c4595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Compute basic dataset sizes\n",
    "num_users = int(max(train_df[\"sid\"].max(), valid_df[\"sid\"].max()) + 1)\n",
    "num_items = int(max(train_df[\"pid\"].max(), valid_df[\"pid\"].max()) + 1)\n",
    "\n",
    "# 2) Choose your NCF hyper-parameters\n",
    "params = {\n",
    "    \"num_users\": num_users,\n",
    "    \"num_items\": num_items,\n",
    "    \"mf_dim\": 64,                     # size of the MF embeddings\n",
    "    \"model_layers\": [64, 32, 16, 8],  # MLP layer sizes\n",
    "    \"mf_regularization\": 0.0,\n",
    "    \"mlp_reg_layers\":    [0.0, 0.0, 0.0, 0.0],\n",
    "    \"batch_size\": 1024,\n",
    "    \"keras_use_ctl\": False,           # whether to use the official repo's Control-Flow-Trick layers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2421b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "824/827 [============================>.] - ETA: 0s - loss: 2.1339"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 18:15:24.274492: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype float and shape [282047]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "827/827 [==============================] - 33s 25ms/step - loss: 2.1302 - val_loss: 0.8147\n",
      "Epoch 2/5\n",
      "827/827 [==============================] - 21s 21ms/step - loss: 0.7657 - val_loss: 0.7627\n",
      "Epoch 3/5\n",
      "827/827 [==============================] - 23s 18ms/step - loss: 0.5503 - val_loss: 0.8069\n",
      "Epoch 4/5\n",
      "827/827 [==============================] - 16s 17ms/step - loss: 0.3974 - val_loss: 0.8756\n",
      "Epoch 5/5\n",
      "827/827 [==============================] - 30s 22ms/step - loss: 0.3266 - val_loss: 0.9359\n",
      "276/276 [==============================] - 2s 3ms/step\n",
      "Validation RMSE: 0.9674\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = build_ncf_regression_model(params)                   \n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",                                         \n",
    ")\n",
    "\n",
    "# 3) Train\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,                                           \n",
    ")\n",
    "\n",
    "# 4) Define the prediction function\n",
    "def pred_fn(sids: np.ndarray, pids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes arrays of user IDs and item IDs and returns\n",
    "    a flat numpy array of predicted ratings.\n",
    "    \"\"\"\n",
    "    # Keras wants a dict of inputs named exactly as your Input layers:\n",
    "    inputs = {\n",
    "        \"sid\":  sids.astype(np.int32),\n",
    "        \"pid\":  pids.astype(np.int32),\n",
    "        # we don’t need masks/labels here\n",
    "    }\n",
    "    # model.predict returns shape (batch,1), so we .flatten()\n",
    "    preds = model.predict(inputs, batch_size=1024)\n",
    "    return preds.flatten()\n",
    "\n",
    "# 5) Compute validation RMSE\n",
    "rmse = evaluate(valid_df, pred_fn)\n",
    "print(f\"Validation RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c42d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
