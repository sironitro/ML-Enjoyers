{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c423bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from typing import Callable, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2368f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "\n",
    "def read_data_df() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads in data and splits it into training and validation sets with a 75/25 split.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_ratings.csv\"))\n",
    "\n",
    "    # Split sid_pid into sid and pid columns\n",
    "    df[[\"sid\", \"pid\"]] = df[\"sid_pid\"].str.split(\"_\", expand=True)\n",
    "    df = df.drop(\"sid_pid\", axis=1)\n",
    "    df[\"sid\"] = df[\"sid\"].astype(int)\n",
    "    df[\"pid\"] = df[\"pid\"].astype(int)\n",
    "    \n",
    "    # Split into train and validation dataset\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.25)\n",
    "    return train_df, valid_df\n",
    "\n",
    "# Read wishlist data\n",
    "def read_tbr_data() -> pd.DataFrame:\n",
    "    \"\"\"Reads the to-be-read (wishlist) data.\"\"\"\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, \"train_tbr.csv\"))\n",
    "    # No need to split sid_pid since columns are already separate\n",
    "    df[[\"sid\", \"pid\"]] = df[[\"sid\", \"pid\"]].astype(int)\n",
    "    return df\n",
    "\n",
    "def evaluate(valid_df: pd.DataFrame, pred_fn: Callable[[np.ndarray, np.ndarray], np.ndarray]) -> float:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        valid_df: Validation data, returned from read_data_df for example.\n",
    "        pred_fn: Function that takes in arrays of sid and pid and outputs their rating predictions.\n",
    "\n",
    "    Outputs: Validation RMSE\n",
    "    \"\"\"\n",
    "    \n",
    "    preds = pred_fn(valid_df[\"sid\"].values, valid_df[\"pid\"].values)\n",
    "    return root_mean_squared_error(valid_df[\"rating\"].values, preds)\n",
    "\n",
    "def evaluate_implicit_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for user_ids, item_ids, author_ids, venue_ids, ratings, _ in data_loader:\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            author_ids = author_ids.to(device)\n",
    "            venue_ids = venue_ids.to(device)\n",
    "            \n",
    "            predictions = model(user_ids, item_ids, author_ids, venue_ids)\n",
    "            \n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_targets.append(ratings.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    rmse = root_mean_squared_error(all_targets, all_preds)\n",
    "    return rmse\n",
    "\n",
    "def predict_ratings(model, user_ids, item_ids, author_ids, venue_ids, device):\n",
    "    \"\"\"Function to make prediction for evaluation\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        user_tensor = torch.from_numpy(user_ids).to(device)\n",
    "        item_tensor = torch.from_numpy(item_ids).to(device)\n",
    "        author_tensor = torch.from_numpy(author_ids).to(device)\n",
    "        venue_tensor = torch.from_numpy(venue_ids).to(device)\n",
    "        \n",
    "        predictions = model(user_tensor, item_tensor, author_tensor, venue_tensor).cpu().numpy()\n",
    "    \n",
    "    return np.clip(predictions, 1, 5)\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sids = df['sid'].values.astype(np.int64)\n",
    "        self.pids = df['pid'].values.astype(np.int64)\n",
    "        self.ratings = df['rating'].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sids[idx], self.pids[idx], self.ratings[idx]\n",
    "\n",
    "    \n",
    "class ImplicitFeedbackDataset(Dataset):\n",
    "    \"\"\"Dataset for both explicit ratings and implicit feedback\"\"\"\n",
    "    def __init__(self, explicit_df, implicit_df=None, author_map=None, venue_map=None):\n",
    "        self.sids = explicit_df['sid'].values.astype(np.int64)\n",
    "        self.pids = explicit_df['pid'].values.astype(np.int64)\n",
    "        \n",
    "        # Get ratings from explicit feedback\n",
    "        self.ratings = explicit_df['rating'].values.astype(np.float32)\n",
    "        \n",
    "        # Get implicit feedback (1 for items in wishlist, 0 otherwise)\n",
    "        if implicit_df is not None:\n",
    "            # Create mapping of (sid, pid) to implicit feedback\n",
    "            implicit_map = {(row['sid'], row['pid']): 1 for _, row in implicit_df.iterrows()}\n",
    "            \n",
    "            # Get implicit feedback for each (sid, pid) pair in explicit_df\n",
    "            self.implicit_feedback = np.array([\n",
    "                implicit_map.get((sid, pid), 0) \n",
    "                for sid, pid in zip(self.sids, self.pids)\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            # If no implicit feedback, use zeros\n",
    "            self.implicit_feedback = np.zeros_like(self.ratings)\n",
    "            \n",
    "        # Get author and venue information if provided\n",
    "        if author_map is not None:\n",
    "            self.author_ids = np.array([author_map.get(pid, 0) for pid in self.pids], dtype=np.int64)\n",
    "        else:\n",
    "            self.author_ids = np.zeros_like(self.pids)\n",
    "            \n",
    "        if venue_map is not None:\n",
    "            self.venue_ids = np.array([venue_map.get(pid, 0) for pid in self.pids], dtype=np.int64)\n",
    "        else:\n",
    "            self.venue_ids = np.zeros_like(self.pids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.sids[idx], \n",
    "            self.pids[idx], \n",
    "            self.author_ids[idx], \n",
    "            self.venue_ids[idx],\n",
    "            self.ratings[idx], \n",
    "            self.implicit_feedback[idx]\n",
    "        )\n",
    "\n",
    "class NeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim=32, mlp_layer_sizes=[64,32,16,8], dropout=0.2):\n",
    "        super().__init__()\n",
    "        # GMF embeddings\n",
    "        self.user_gmf = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_gmf = nn.Embedding(num_items, mf_dim)\n",
    "        # MLP embeddings\n",
    "        self.user_mlp = nn.Embedding(num_users, mlp_layer_sizes[0])\n",
    "        self.item_mlp = nn.Embedding(num_items, mlp_layer_sizes[0])\n",
    "        \n",
    "        # MLP layers\n",
    "        mlp_layers = []\n",
    "        in_size = mlp_layer_sizes[0] * 2\n",
    "        for out_size in mlp_layer_sizes[1:]:\n",
    "            mlp_layers += [nn.Dropout(dropout), nn.Linear(in_size, out_size), nn.ReLU()]\n",
    "            in_size = out_size\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # Final prediction\n",
    "        self.predict = nn.Linear(mf_dim + mlp_layer_sizes[-1], 1)\n",
    "        \n",
    "        # Initialization\n",
    "        nn.init.normal_(self.user_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_mlp.weight, std=0.01)\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.kaiming_uniform_(self.predict.weight, a=1, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        gmf = self.user_gmf(u) * self.item_gmf(i)\n",
    "        mlp = self.mlp(torch.cat([self.user_mlp(u), self.item_mlp(i)], dim=1))\n",
    "        x = torch.cat([gmf, mlp], dim=1)\n",
    "        return self.predict(x).squeeze()\n",
    "\n",
    "class EnhancedNeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim=64, mlp_layer_sizes=[128,64,32], dropout=0.3):\n",
    "        super().__init__()\n",
    "        # GMF embeddings\n",
    "        self.user_gmf = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_gmf = nn.Embedding(num_items, mf_dim)\n",
    "        # MLP embeddings\n",
    "        self.user_mlp = nn.Embedding(num_users, mlp_layer_sizes[0])\n",
    "        self.item_mlp = nn.Embedding(num_items, mlp_layer_sizes[0])\n",
    "        \n",
    "        # Add user and item bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Improved MLP with BatchNorm\n",
    "        mlp_layers = []\n",
    "        in_size = mlp_layer_sizes[0] * 2\n",
    "        for out_size in mlp_layer_sizes[1:]:\n",
    "            mlp_layers += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.BatchNorm1d(out_size),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            in_size = out_size\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # Final prediction\n",
    "        self.predict = nn.Linear(mf_dim + mlp_layer_sizes[-1], 1)\n",
    "        \n",
    "        # Initialization\n",
    "        nn.init.normal_(self.user_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_mlp.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.kaiming_uniform_(self.predict.weight, a=1, nonlinearity='sigmoid')\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        # GMF path\n",
    "        gmf = self.user_gmf(u) * self.item_gmf(i)\n",
    "        # MLP path\n",
    "        mlp = self.mlp(torch.cat([self.user_mlp(u), self.item_mlp(i)], dim=1))\n",
    "        # Combine paths\n",
    "        x = torch.cat([gmf, mlp], dim=1)\n",
    "        base_pred = self.predict(x).squeeze()\n",
    "        # Add bias terms\n",
    "        u_bias = self.user_bias(u).squeeze()\n",
    "        i_bias = self.item_bias(i).squeeze()\n",
    "        return base_pred + u_bias + i_bias\n",
    "\n",
    "class ImplicitEnhancedNeuMF(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_users, \n",
    "        num_items, \n",
    "        num_authors,\n",
    "        num_venues, \n",
    "        embedding_dim=64, \n",
    "        mlp_dims=[128, 64, 32], \n",
    "        dropout_rate=0.3,\n",
    "        implicit_weight=0.5  # Weight for implicit feedback contribution\n",
    "    ):\n",
    "        super(ImplicitEnhancedNeuMF, self).__init__()\n",
    "        \n",
    "        # Explicit feedback pathway\n",
    "        self.user_gmf = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_gmf = nn.Embedding(num_items, embedding_dim)\n",
    "        self.user_mlp = nn.Embedding(num_users, mlp_dims[0])\n",
    "        self.item_mlp = nn.Embedding(num_items, mlp_dims[0])\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Explicit MLP pathway\n",
    "        explicit_mlp_layers = []\n",
    "        in_size = mlp_dims[0] * 2\n",
    "        for out_size in mlp_dims[1:]:\n",
    "            explicit_mlp_layers += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.BatchNorm1d(out_size),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ]\n",
    "            in_size = out_size\n",
    "        self.explicit_mlp = nn.Sequential(*explicit_mlp_layers)\n",
    "        \n",
    "        # Final prediction for explicit pathway\n",
    "        self.explicit_predict = nn.Linear(embedding_dim + mlp_dims[-1], 1)\n",
    "        \n",
    "        # Implicit feedback pathway\n",
    "        self.user_implicit_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_implicit_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # Author and venue embeddings for metadata\n",
    "        self.author_embedding = nn.Embedding(num_authors, embedding_dim)\n",
    "        self.venue_embedding = nn.Embedding(num_venues, embedding_dim)\n",
    "        \n",
    "        # MLP for implicit feedback with metadata\n",
    "        implicit_input_dim = embedding_dim * 4  # User, item, author, venue\n",
    "        implicit_mlp_layers = []\n",
    "        in_size = implicit_input_dim\n",
    "        for out_size in mlp_dims:\n",
    "            implicit_mlp_layers += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.BatchNorm1d(out_size),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ]\n",
    "            in_size = out_size\n",
    "        implicit_mlp_layers.append(nn.Linear(in_size, 1))\n",
    "        self.implicit_mlp = nn.Sequential(*implicit_mlp_layers)\n",
    "        \n",
    "        # Integration layer\n",
    "        self.integration_layer = nn.Linear(2, 1)\n",
    "        \n",
    "        # Weight for balancing explicit and implicit signals\n",
    "        self.implicit_weight = implicit_weight\n",
    "        \n",
    "        # Initialization\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Initialize embeddings\n",
    "        for module in [self.user_gmf, self.item_gmf, self.user_mlp, self.item_mlp,\n",
    "                     self.user_implicit_embedding, self.item_implicit_embedding,\n",
    "                     self.author_embedding, self.venue_embedding]:\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        \n",
    "        # Initialize bias\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "        # Initialize linear layers\n",
    "        for m in self.explicit_mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                \n",
    "        for i, m in enumerate(self.implicit_mlp):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if i == len(self.implicit_mlp) - 1:  # Output layer\n",
    "                    nn.init.kaiming_uniform_(m.weight, a=1, nonlinearity='sigmoid')\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.explicit_predict.weight)\n",
    "        nn.init.xavier_uniform_(self.integration_layer.weight)\n",
    "        nn.init.zeros_(self.integration_layer.bias)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices, author_indices, venue_indices):\n",
    "        # Explicit feedback pathway (similar to EnhancedNeuMF)\n",
    "        gmf = self.user_gmf(user_indices) * self.item_gmf(item_indices)\n",
    "        mlp = self.explicit_mlp(torch.cat([self.user_mlp(user_indices), self.item_mlp(item_indices)], dim=1))\n",
    "        \n",
    "        # Combine GMF and MLP for explicit prediction\n",
    "        explicit_concat = torch.cat([gmf, mlp], dim=1)\n",
    "        explicit_score = self.explicit_predict(explicit_concat).squeeze()\n",
    "        \n",
    "        # Add bias terms to explicit score\n",
    "        u_bias = self.user_bias(user_indices).squeeze()\n",
    "        i_bias = self.item_bias(item_indices).squeeze()\n",
    "        explicit_score = explicit_score + u_bias + i_bias\n",
    "        \n",
    "        # Implicit feedback pathway\n",
    "        user_implicit = self.user_implicit_embedding(user_indices)\n",
    "        item_implicit = self.item_implicit_embedding(item_indices)\n",
    "        author_embed = self.author_embedding(author_indices)\n",
    "        venue_embed = self.venue_embedding(venue_indices)\n",
    "        \n",
    "        # Process implicit feedback with metadata\n",
    "        implicit_concat = torch.cat([user_implicit, item_implicit, author_embed, venue_embed], dim=1)\n",
    "        implicit_score = self.implicit_mlp(implicit_concat).squeeze()\n",
    "        \n",
    "        # Combine explicit and implicit scores\n",
    "        combined_scores = torch.cat([\n",
    "            explicit_score.unsqueeze(1), \n",
    "            implicit_score.unsqueeze(1)\n",
    "        ], dim=1)\n",
    "        \n",
    "        final_score = self.integration_layer(combined_scores).squeeze()\n",
    "        \n",
    "        return torch.clamp(final_score, 1.0, 5.0)\n",
    "    \n",
    "    def predict(self, user_indices, item_indices, author_indices, venue_indices):\n",
    "        return self.forward(user_indices, item_indices, author_indices, venue_indices)\n",
    "\n",
    "\n",
    "class AttentionNeuMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim=64, mlp_layer_sizes=[128,64,32], dropout=0.3):\n",
    "        super().__init__()\n",
    "        # GMF embeddings\n",
    "        self.user_gmf = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_gmf = nn.Embedding(num_items, mf_dim)\n",
    "        # MLP embeddings\n",
    "        self.user_mlp = nn.Embedding(num_users, mlp_layer_sizes[0])\n",
    "        self.item_mlp = nn.Embedding(num_items, mlp_layer_sizes[0])\n",
    "        \n",
    "        # Add user and item bias terms\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Improved MLP with BatchNorm\n",
    "        mlp_layers = []\n",
    "        in_size = mlp_layer_sizes[0] * 2\n",
    "        for out_size in mlp_layer_sizes[1:]:\n",
    "            mlp_layers += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.BatchNorm1d(out_size),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            in_size = out_size\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(mf_dim + mlp_layer_sizes[-1], 2)\n",
    "        \n",
    "        # Final prediction\n",
    "        self.predict = nn.Linear(mf_dim + mlp_layer_sizes[-1], 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.normal_(self.user_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_gmf.weight, std=0.01)\n",
    "        nn.init.normal_(self.user_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_mlp.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        nn.init.xavier_uniform_(self.attention.weight)\n",
    "        nn.init.kaiming_uniform_(self.predict.weight, a=1, nonlinearity='sigmoid')\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        # GMF path\n",
    "        gmf = self.user_gmf(u) * self.item_gmf(i)\n",
    "        # MLP path\n",
    "        mlp = self.mlp(torch.cat([self.user_mlp(u), self.item_mlp(i)], dim=1))\n",
    "        \n",
    "        # Concatenate for attention\n",
    "        concat = torch.cat([gmf, mlp], dim=1)\n",
    "        \n",
    "        # Attention weights\n",
    "        att_weights = F.softmax(self.attention(concat), dim=1)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        weighted_gmf = gmf * att_weights[:, 0:1]\n",
    "        weighted_mlp = mlp * att_weights[:, 1:2]\n",
    "        \n",
    "        # Combine for prediction\n",
    "        combined = torch.cat([weighted_gmf, weighted_mlp], dim=1)\n",
    "        \n",
    "        # Get prediction with bias terms\n",
    "        base_pred = self.predict(combined).squeeze()\n",
    "        u_bias = self.user_bias(u).squeeze()\n",
    "        i_bias = self.item_bias(i).squeeze()\n",
    "        \n",
    "        return base_pred + u_bias + i_bias\n",
    "\n",
    "class ImprovedNeuMF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        mf_dim: int = 64,\n",
    "        mlp_dim: int = 128,\n",
    "        mlp_layers: list = [128, 64, 32],\n",
    "        dropout: float = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Separate embeddings for MF and MLP paths (from NeuMFConcat)\n",
    "        self.user_emb_mf = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_emb_mf = nn.Embedding(num_items, mf_dim)\n",
    "        self.user_emb_mlp = nn.Embedding(num_users, mlp_dim)\n",
    "        self.item_emb_mlp = nn.Embedding(num_items, mlp_dim)\n",
    "        \n",
    "        # Bias terms (from EnhancedNeuMF)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # MLP tower with BatchNorm and LeakyReLU\n",
    "        layers = []\n",
    "        in_size = mlp_dim * 2\n",
    "        for out_size in mlp_layers:\n",
    "            layers += [\n",
    "                nn.Linear(in_size, out_size),\n",
    "                nn.BatchNorm1d(out_size),\n",
    "                nn.LeakyReLU(0.1),\n",
    "                nn.Dropout(dropout)\n",
    "            ]\n",
    "            in_size = out_size\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        # Final fusion layer\n",
    "        self.fusion = nn.Linear(mf_dim + mlp_layers[-1], 1)\n",
    "        \n",
    "        # Proper initialization\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.user_emb_mf.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb_mf.weight, std=0.01)\n",
    "        nn.init.normal_(self.user_emb_mlp.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_emb_mlp.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        # Initialize MLP and fusion layers\n",
    "        for m in self.mlp:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.kaiming_uniform_(self.fusion.weight, a=0.01, nonlinearity='sigmoid')\n",
    "    \n",
    "    def forward(self, user_idx, item_idx):\n",
    "        # MF path\n",
    "        mf_u = self.user_emb_mf(user_idx)\n",
    "        mf_i = self.item_emb_mf(item_idx)\n",
    "        mf_vector = mf_u * mf_i  # element-wise multiplication\n",
    "        \n",
    "        # MLP path\n",
    "        mlp_u = self.user_emb_mlp(user_idx)\n",
    "        mlp_i = self.item_emb_mlp(item_idx)\n",
    "        mlp_vector = torch.cat([mlp_u, mlp_i], dim=1)\n",
    "        mlp_vector = self.mlp(mlp_vector)\n",
    "        \n",
    "        # Fusion\n",
    "        concat = torch.cat([mf_vector, mlp_vector], dim=1)\n",
    "        prediction = self.fusion(concat).squeeze()\n",
    "        \n",
    "        # Add bias terms\n",
    "        u_bias = self.user_bias(user_idx).squeeze()\n",
    "        i_bias = self.item_bias(item_idx).squeeze()\n",
    "        \n",
    "        return prediction + u_bias + i_bias\n",
    "\n",
    "class NeuMFRegressor(nn.Module):\n",
    "    def __init__(self, num_users, num_items, mf_dim=32, mlp_sizes=[128,64,32],\n",
    "                 gm_dropout=0.2, mlp_dropout=0.3, min_rating=1.0, max_rating=5.0):\n",
    "        super().__init__()\n",
    "        # GMF branch\n",
    "        self.user_gmf      = nn.Embedding(num_users, mf_dim)\n",
    "        self.item_gmf      = nn.Embedding(num_items, mf_dim)\n",
    "        self.gmf_dropout   = nn.Dropout(gm_dropout)\n",
    "        # MLP branch\n",
    "        self.user_mlp      = nn.Embedding(num_users, mlp_sizes[0])\n",
    "        self.item_mlp      = nn.Embedding(num_items, mlp_sizes[0])\n",
    "        layers = []\n",
    "        in_dim = mlp_sizes[0]*2\n",
    "        for d in mlp_sizes[1:]:\n",
    "            layers += [nn.Linear(in_dim, d),\n",
    "                       nn.LayerNorm(d),\n",
    "                       nn.LeakyReLU(0.1),\n",
    "                       nn.Dropout(mlp_dropout)]\n",
    "            in_dim = d\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        # fusion gate, final predict, biases\n",
    "        self.alpha         = nn.Parameter(torch.tensor(0.5))\n",
    "        self.predict       = nn.Linear(mf_dim + mlp_sizes[-1], 1)\n",
    "        self.user_bias     = nn.Embedding(num_users, 1)\n",
    "        self.item_bias     = nn.Embedding(num_items, 1)\n",
    "        self.global_bias   = nn.Parameter(torch.zeros(1))\n",
    "        self.min_rating    = min_rating\n",
    "        self.max_rating    = max_rating\n",
    "\n",
    "        # Xavier init\n",
    "        for emb in [self.user_gmf, self.item_gmf,\n",
    "                    self.user_mlp, self.item_mlp]:\n",
    "            nn.init.xavier_uniform_(emb.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        nn.init.xavier_uniform_(self.predict.weight)\n",
    "\n",
    "    def forward(self, u, i):\n",
    "        g = self.gmf_dropout(self.user_gmf(u) * self.item_gmf(i))\n",
    "        m = self.mlp(torch.cat([self.user_mlp(u), self.item_mlp(i)], 1))\n",
    "        x = torch.cat([self.alpha*g, (1-self.alpha)*m], 1)\n",
    "        base = self.predict(x).squeeze()\n",
    "        out  = base \\\n",
    "             + self.user_bias(u).squeeze() \\\n",
    "             + self.item_bias(i).squeeze() \\\n",
    "             + self.global_bias\n",
    "        return torch.clamp(out, self.min_rating, self.max_rating)\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Apply user-specific normalization to ratings\"\"\"\n",
    "    # Get global mean\n",
    "    global_mean = df['rating'].mean()\n",
    "    print(f\"Global mean rating: {global_mean:.4f}\")\n",
    "    \n",
    "    # Get user biases (average rating deviation from global mean)\n",
    "    user_biases = df.groupby('sid')['rating'].mean() - global_mean\n",
    "    \n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_norm = df.copy()\n",
    "    \n",
    "    # Normalize ratings by user bias\n",
    "    def normalize_rating(row):\n",
    "        user_id = row['sid']\n",
    "        return row['rating'] - user_biases.get(user_id, 0)\n",
    "    \n",
    "    df_norm['rating'] = df_norm.apply(normalize_rating, axis=1)\n",
    "    \n",
    "    return df_norm, user_biases, global_mean\n",
    "\n",
    "def prepare_implicit_feedback(ratings_df, tbr_df, threshold=3.5):\n",
    "    \"\"\"\n",
    "    Prepare implicit feedback from both ratings and TBR datasets\n",
    "    \n",
    "    Args:\n",
    "        ratings_df: DataFrame with explicit ratings\n",
    "        tbr_df: DataFrame with to-be-read items\n",
    "        threshold: Rating threshold to consider as positive feedback (default: 3.5)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with user_id, item_id, and implicit feedback\n",
    "    \"\"\"\n",
    "    # Get positive implicit feedback from high ratings\n",
    "    positive_ratings = ratings_df[ratings_df['rating'] >= threshold][['sid', 'pid']]\n",
    "    positive_ratings['implicit'] = 1\n",
    "    \n",
    "    # Get implicit feedback from TBR items\n",
    "    tbr_implicit = tbr_df[['sid', 'pid']].copy()\n",
    "    tbr_implicit['implicit'] = 1\n",
    "    \n",
    "    # Combine both sources and remove duplicates\n",
    "    implicit_feedback = pd.concat([positive_ratings, tbr_implicit]).drop_duplicates(['sid', 'pid'])\n",
    "    \n",
    "    return implicit_feedback\n",
    "\n",
    "def get_metadata_maps(ratings_df):\n",
    "    \"\"\"\n",
    "    Create author and venue mappings from item IDs\n",
    "    This is a placeholder - in a real scenario, you'd load actual metadata\n",
    "    \n",
    "    Returns:\n",
    "        author_map, venue_map: Dictionaries mapping item IDs to author/venue IDs\n",
    "    \"\"\"\n",
    "    # In a real scenario, you'd load actual metadata\n",
    "    # Here we're just creating placeholders based on item IDs\n",
    "    items = ratings_df['pid'].unique()\n",
    "    \n",
    "    # Assign random author and venue IDs\n",
    "    # In a real scenario, these would be actual mappings\n",
    "    author_map = {pid: np.random.randint(1, 100) for pid in items}\n",
    "    venue_map = {pid: np.random.randint(1, 50) for pid in items}\n",
    "    \n",
    "    return author_map, venue_map\n",
    "\n",
    "def train(model, train_df, valid_df, loader, optimizer, criterion, device, epochs=20):\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training step\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for sids, pids, ratings in loader:\n",
    "            sids, pids, ratings = sids.to(device), pids.to(device), ratings.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(sids, pids)\n",
    "            loss = criterion(preds, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(ratings)\n",
    "            \n",
    "        # Create prediction function for evaluation\n",
    "        def pred_fn(s, p):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(\n",
    "                    torch.from_numpy(s).to(device), \n",
    "                    torch.from_numpy(p).to(device)\n",
    "                ).detach().cpu().numpy()\n",
    "            return np.clip(preds, 1, 5)\n",
    "        \n",
    "        # Evaluate on both train and validation sets\n",
    "        train_rmse = evaluate(train_df, pred_fn)\n",
    "        valid_rmse = evaluate(valid_df, pred_fn)\n",
    "        \n",
    "        # Learning rate scheduling (if you have a scheduler)\n",
    "        # scheduler.step(valid_rmse)\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} — Train RMSE: {train_rmse:.4f}, Valid RMSE: {valid_rmse:.4f}\")\n",
    "        \n",
    "        if valid_rmse < best_rmse:\n",
    "            best_rmse = valid_rmse\n",
    "            torch.save(model.state_dict(), 'best_ncf.pth')\n",
    "    \n",
    "    print(f\"\\nBest Val RMSE: {best_rmse:.4f}\")\n",
    "    return best_rmse\n",
    "\n",
    "def train_enhanced(model, train_df, valid_df, loader, optimizer, criterion, device, \n",
    "                  epochs=20, patience=5, clip_norm=1.0):\n",
    "    best_rmse = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        # Training step\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for sids, pids, ratings in loader:\n",
    "            sids, pids, ratings = sids.to(device), pids.to(device), ratings.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(sids, pids)\n",
    "            loss = criterion(preds, ratings)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(ratings)\n",
    "            \n",
    "        # Create prediction function for evaluation\n",
    "        def pred_fn(s, p):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds = model(\n",
    "                    torch.from_numpy(s).to(device), \n",
    "                    torch.from_numpy(p).to(device)\n",
    "                ).detach().cpu().numpy()\n",
    "            return np.clip(preds, 1, 5)\n",
    "        \n",
    "        # Evaluate on both train and validation sets\n",
    "        train_rmse = evaluate(train_df, pred_fn)\n",
    "        valid_rmse = evaluate(valid_df, pred_fn)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(valid_rmse)\n",
    "        \n",
    "        print(f\"Epoch {epoch:02d} — Train RMSE: {train_rmse:.4f}, Valid RMSE: {valid_rmse:.4f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        if valid_rmse < best_rmse:\n",
    "            best_rmse = valid_rmse\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_ncf.pth')\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nBest Val RMSE: {best_rmse:.4f}\")\n",
    "    return best_rmse\n",
    "\n",
    "def train_implicit_model(model, train_loader, valid_loader, optimizer, device, \n",
    "                        explicit_criterion=nn.MSELoss(), \n",
    "                        implicit_criterion=nn.BCEWithLogitsLoss(),\n",
    "                        epochs=20, patience=5):\n",
    "    \"\"\"\n",
    "    Training function for the implicit feedback enhanced model\n",
    "    \"\"\"\n",
    "    best_rmse = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for user_ids, item_ids, author_ids, venue_ids, ratings, implicit_feedback in train_loader:\n",
    "            user_ids = user_ids.to(device)\n",
    "            item_ids = item_ids.to(device)\n",
    "            author_ids = author_ids.to(device)\n",
    "            venue_ids = venue_ids.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            implicit_feedback = implicit_feedback.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(user_ids, item_ids, author_ids, venue_ids)\n",
    "            \n",
    "            # Calculate explicit loss (MSE for ratings)\n",
    "            explicit_loss = explicit_criterion(predictions, ratings)\n",
    "            \n",
    "            # Overall loss is primarily MSE rating loss\n",
    "            loss = explicit_loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_rmse = evaluate_implicit_model(model, valid_loader, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_rmse)\n",
    "        \n",
    "        # Print progress\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch}/{epochs} - Train Loss: {avg_loss:.4f}, Val RMSE: {val_rmse:.4f}, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_implicit_model.pth')\n",
    "            print(f\"Model improved - saving checkpoint (RMSE: {best_rmse:.4f})\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"No improvement for {early_stop_counter} epochs\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_implicit_model.pth'))\n",
    "    return model, best_rmse\n",
    "\n",
    "def create_ensemble(num_users, num_items, device):\n",
    "    \"\"\"Create an ensemble of models with different configurations\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # Config 1: Default with more capacity\n",
    "    model1 = EnhancedNeuMF(\n",
    "        num_users, num_items, \n",
    "        mf_dim=64, \n",
    "        mlp_layer_sizes=[128, 64, 32], \n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Config 2: Smaller with less dropout\n",
    "    model2 = EnhancedNeuMF(\n",
    "        num_users, num_items, \n",
    "        mf_dim=32, \n",
    "        mlp_layer_sizes=[64, 32, 16], \n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    \n",
    "    # Config 3: Attention-based model\n",
    "    model3 = AttentionNeuMF(\n",
    "        num_users, num_items,\n",
    "        mf_dim=64,\n",
    "        mlp_layer_sizes=[128, 64, 32],\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    return [model1, model2, model3]\n",
    "\n",
    "def ensemble_predict(models, sids, pids, device):\n",
    "    \"\"\"Make predictions with an ensemble of models\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            s_tensor = torch.from_numpy(sids).to(device) \n",
    "            p_tensor = torch.from_numpy(pids).to(device)\n",
    "            preds = model(s_tensor, p_tensor).detach().cpu().numpy()\n",
    "        predictions.append(preds)\n",
    "        \n",
    "    # Average predictions\n",
    "    ensemble_preds = np.mean(predictions, axis=0)\n",
    "    return np.clip(ensemble_preds, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a95b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70fa302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ratings data\n",
    "train_df, valid_df = read_data_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643bff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tbr data\n",
    "tbr_data = read_tbr_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e7ba9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mean rating: 3.8179\n",
      "Num users: 10000, Num items: 1000\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "train_df_norm, user_biases, global_mean = preprocess_data(train_df)\n",
    "valid_df_norm = valid_df.copy()\n",
    "valid_df_norm['rating'] = valid_df_norm.apply(\n",
    "    lambda row: row['rating'] - user_biases.get(row['sid'], 0), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Determine number of users and items\n",
    "num_users = train_df['sid'].max() + 1\n",
    "num_items = train_df['pid'].max() + 1\n",
    "print(f\"Num users: {num_users}, Num items: {num_items}\")\n",
    "\n",
    "# Prepare data loader with normalized data\n",
    "train_loader = DataLoader(\n",
    "    RatingDataset(train_df_norm), \n",
    "    batch_size=1024, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1fa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — Train RMSE: 0.9083, Valid RMSE: 0.9246, LR: 0.000500\n",
      "Epoch 02 — Train RMSE: 0.8742, Valid RMSE: 0.8956, LR: 0.000500\n",
      "Epoch 03 — Train RMSE: 0.8457, Valid RMSE: 0.8806, LR: 0.000500\n",
      "Epoch 04 — Train RMSE: 0.8222, Valid RMSE: 0.8748, LR: 0.000500\n",
      "Epoch 05 — Train RMSE: 0.7979, Valid RMSE: 0.8710, LR: 0.000500\n",
      "Epoch 06 — Train RMSE: 0.7726, Valid RMSE: 0.8696, LR: 0.000500\n",
      "Epoch 07 — Train RMSE: 0.7460, Valid RMSE: 0.8707, LR: 0.000500\n",
      "Epoch 08 — Train RMSE: 0.7171, Valid RMSE: 0.8733, LR: 0.000500\n",
      "Epoch 09 — Train RMSE: 0.6878, Valid RMSE: 0.8786, LR: 0.000500\n",
      "Epoch 10 — Train RMSE: 0.6550, Valid RMSE: 0.8850, LR: 0.000250\n",
      "Epoch 11 — Train RMSE: 0.6279, Valid RMSE: 0.8889, LR: 0.000250\n",
      "Early stopping triggered after 11 epochs\n",
      "\n",
      "Best Val RMSE: 0.8696\n"
     ]
    }
   ],
   "source": [
    "# Use the enhanced model\n",
    "model = EnhancedNeuMF(num_users, num_items).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train with the enhanced training function\n",
    "best_rmse = train_enhanced(\n",
    "    model=model,\n",
    "    train_df=train_df_norm,\n",
    "    valid_df=valid_df_norm,\n",
    "    loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    epochs=20,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f12a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — Train RMSE: 0.8773, Valid RMSE: 0.8931, LR: 0.000500\n",
      "Epoch 02 — Train RMSE: 0.8653, Valid RMSE: 0.8883, LR: 0.000500\n",
      "Epoch 03 — Train RMSE: 0.8364, Valid RMSE: 0.8806, LR: 0.000500\n",
      "Epoch 04 — Train RMSE: 0.7805, Valid RMSE: 0.8730, LR: 0.000500\n",
      "Epoch 05 — Train RMSE: 0.7258, Valid RMSE: 0.8805, LR: 0.000500\n",
      "Epoch 06 — Train RMSE: 0.6917, Valid RMSE: 0.8916, LR: 0.000500\n",
      "Epoch 07 — Train RMSE: 0.6689, Valid RMSE: 0.9014, LR: 0.000500\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = NeuMFRegressor(num_users, num_items).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_rmse = train_enhanced(\n",
    "    model=model,\n",
    "    train_df=train_df_norm,\n",
    "    valid_df=valid_df_norm,\n",
    "    loader=train_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    epochs=30,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82cfd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.output_layer = nn.Linear(embedding_dim, 1)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        nn.init.kaiming_uniform_(self.output_layer.weight)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embed = self.user_embedding(user_indices)\n",
    "        item_embed = self.item_embedding(item_indices)\n",
    "        element_product = user_embed * item_embed\n",
    "        \n",
    "        prediction = self.output_layer(element_product).squeeze()\n",
    "        prediction += self.user_bias(user_indices).squeeze()\n",
    "        prediction += self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        return torch.clamp(prediction, 1.0, 5.0)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, \n",
    "                 layers=[128, 64, 32], dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        \n",
    "        # MLP layers\n",
    "        self.mlp_layers = []\n",
    "        layer_sizes = [embedding_dim * 2] + layers\n",
    "        \n",
    "        mlp_modules = []\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            mlp_modules.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            mlp_modules.append(nn.BatchNorm1d(layer_sizes[i+1]))\n",
    "            mlp_modules.append(nn.LeakyReLU(0.1))\n",
    "            mlp_modules.append(nn.Dropout(dropout))\n",
    "            \n",
    "        self.mlp_layers = nn.Sequential(*mlp_modules)\n",
    "        self.output_layer = nn.Linear(layer_sizes[-1], 1)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.01)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.01)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "        \n",
    "        for m in self.mlp_layers:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.kaiming_uniform_(self.output_layer.weight)\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        user_embed = self.user_embedding(user_indices)\n",
    "        item_embed = self.item_embedding(item_indices)\n",
    "        \n",
    "        vector = torch.cat([user_embed, item_embed], dim=-1)\n",
    "        mlp_output = self.mlp_layers(vector)\n",
    "        \n",
    "        prediction = self.output_layer(mlp_output).squeeze()\n",
    "        prediction += self.user_bias(user_indices).squeeze()\n",
    "        prediction += self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        return torch.clamp(prediction, 1.0, 5.0)\n",
    "    \n",
    "class NeuMFPretrainedFusion(nn.Module):\n",
    "    def __init__(self, gmf_model, mlp_model, alpha=0.5):\n",
    "        super().__init__()\n",
    "        # GMF embeddings and output layers (copied from pretrained)\n",
    "        self.gmf_user_embedding = gmf_model.user_embedding\n",
    "        self.gmf_item_embedding = gmf_model.item_embedding\n",
    "        self.gmf_output = gmf_model.output_layer\n",
    "        \n",
    "        # MLP embeddings and layers (copied from pretrained)\n",
    "        self.mlp_user_embedding = mlp_model.user_embedding\n",
    "        self.mlp_item_embedding = mlp_model.item_embedding\n",
    "        self.mlp_layers = mlp_model.mlp_layers\n",
    "        self.mlp_output = mlp_model.output_layer\n",
    "        \n",
    "        # Bias terms\n",
    "        self.user_bias = gmf_model.user_bias\n",
    "        self.item_bias = gmf_model.item_bias\n",
    "        \n",
    "        # Fusion parameter (trainable or fixed)\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha)) if isinstance(alpha, float) else alpha\n",
    "        \n",
    "    def forward(self, user_indices, item_indices):\n",
    "        # GMF path\n",
    "        gmf_user_embed = self.gmf_user_embedding(user_indices)\n",
    "        gmf_item_embed = self.gmf_item_embedding(user_indices)\n",
    "        gmf_vector = gmf_user_embed * gmf_item_embed\n",
    "        gmf_pred = self.gmf_output(gmf_vector)\n",
    "        \n",
    "        # MLP path\n",
    "        mlp_user_embed = self.mlp_user_embedding(user_indices)\n",
    "        mlp_item_embed = self.mlp_item_embedding(item_indices)\n",
    "        mlp_vector = torch.cat([mlp_user_embed, mlp_item_embed], dim=-1)\n",
    "        mlp_vector = self.mlp_layers(mlp_vector)\n",
    "        mlp_pred = self.mlp_output(mlp_vector)\n",
    "        \n",
    "        # Combine predictions with alpha weighting\n",
    "        prediction = self.alpha * gmf_pred + (1 - self.alpha) * mlp_pred\n",
    "        prediction = prediction.squeeze()\n",
    "        \n",
    "        # Add bias terms\n",
    "        prediction += self.user_bias(user_indices).squeeze()\n",
    "        prediction += self.item_bias(item_indices).squeeze()\n",
    "        \n",
    "        return torch.clamp(prediction, 1.0, 5.0)\n",
    "    \n",
    "def train_classic_ncf(num_users, num_items, train_loader, \n",
    "                      train_df, valid_df, device, criterion):\n",
    "    print(\"Step 1: Training GMF model...\")\n",
    "    gmf_model = GMF(num_users, num_items).to(device)\n",
    "    gmf_optimizer = torch.optim.Adam(gmf_model.parameters(), lr=1e-3)\n",
    "    \n",
    "    train_enhanced(\n",
    "        model=gmf_model,\n",
    "        train_df=train_df,\n",
    "        valid_df=valid_df,\n",
    "        loader=train_loader,\n",
    "        optimizer=gmf_optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=20,\n",
    "        patience=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\nStep 2: Training MLP model...\")\n",
    "    mlp_model = MLP(num_users, num_items).to(device)\n",
    "    mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
    "    \n",
    "    train_enhanced(\n",
    "        model=mlp_model,\n",
    "        train_df=train_df,\n",
    "        valid_df=valid_df,\n",
    "        loader=train_loader,\n",
    "        optimizer=mlp_optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=20,\n",
    "        patience=3\n",
    "    )\n",
    "    \n",
    "    # Try different alpha values or make it learnable\n",
    "    print(\"\\nStep 3: Fine-tuning combined model...\")\n",
    "    best_alpha = 0.5\n",
    "    best_rmse = float('inf')\n",
    "    \n",
    "    # Option 1: Grid search alpha\n",
    "    for alpha in [0.3, 0.5, 0.7]:\n",
    "        print(f\"Testing alpha = {alpha}\")\n",
    "        fusion_model = NeuMFPretrainedFusion(\n",
    "            gmf_model, mlp_model, alpha=alpha).to(device)\n",
    "        \n",
    "        # Freeze embedding weights and only train output layers \n",
    "        for param in fusion_model.gmf_user_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in fusion_model.gmf_item_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in fusion_model.mlp_user_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in fusion_model.mlp_item_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        fusion_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, fusion_model.parameters()), \n",
    "            lr=5e-4)\n",
    "        \n",
    "        rmse = train_enhanced(\n",
    "            model=fusion_model,\n",
    "            train_df=train_df,\n",
    "            valid_df=valid_df,\n",
    "            loader=train_loader,\n",
    "            optimizer=fusion_optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            epochs=10,\n",
    "            patience=3\n",
    "        )\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_alpha = alpha\n",
    "    \n",
    "    # Option 2: Learnable alpha\n",
    "    print(\"\\nFinal model with learnable alpha...\")\n",
    "    fusion_model = NeuMFPretrainedFusion(\n",
    "        gmf_model, mlp_model, alpha=best_alpha).to(device)\n",
    "    \n",
    "    # Unfreeze everything for final training\n",
    "    fusion_optimizer = torch.optim.Adam(fusion_model.parameters(), lr=1e-4)\n",
    "    \n",
    "    final_rmse = train_enhanced(\n",
    "        model=fusion_model,\n",
    "        train_df=train_df,\n",
    "        valid_df=valid_df,\n",
    "        loader=train_loader,\n",
    "        optimizer=fusion_optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=15,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    return fusion_model, final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "649ede45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions:\n",
      "  Users: 10000\n",
      "  Items: 1000\n",
      "  Authors: 100\n",
      "  Venues: 50\n",
      "  Training ratings: 846140\n",
      "  Validation ratings: 282047\n",
      "  TBR (wishlist) items: 328839\n",
      "  Combined implicit feedback points: 880622\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Prepare implicit feedback data\n",
    "implicit_data = prepare_implicit_feedback(train_df, tbr_data, threshold=4.0)\n",
    "\n",
    "# Get metadata maps (placeholder function - would use real metadata in production)\n",
    "author_map, venue_map = get_metadata_maps(train_df)\n",
    "\n",
    "# Determine dimensions\n",
    "num_users = max(train_df['sid'].max(), tbr_data['sid'].max()) + 1\n",
    "num_items = max(train_df['pid'].max(), tbr_data['pid'].max()) + 1\n",
    "num_authors = max(author_map.values()) + 1\n",
    "num_venues = max(venue_map.values()) + 1\n",
    "\n",
    "# Print some info\n",
    "print(f\"Dataset dimensions:\")\n",
    "print(f\"  Users: {num_users}\")\n",
    "print(f\"  Items: {num_items}\")\n",
    "print(f\"  Authors: {num_authors}\")\n",
    "print(f\"  Venues: {num_venues}\")\n",
    "print(f\"  Training ratings: {len(train_df)}\")\n",
    "print(f\"  Validation ratings: {len(valid_df)}\")\n",
    "print(f\"  TBR (wishlist) items: {len(tbr_data)}\")\n",
    "print(f\"  Combined implicit feedback points: {len(implicit_data)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImplicitFeedbackDataset(\n",
    "    train_df, \n",
    "    implicit_data, \n",
    "    author_map, \n",
    "    venue_map\n",
    ")\n",
    "\n",
    "valid_dataset = ImplicitFeedbackDataset(\n",
    "    valid_df,\n",
    "    implicit_data,\n",
    "    author_map,\n",
    "    venue_map\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = ImplicitEnhancedNeuMF(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    num_authors=num_authors,\n",
    "    num_venues=num_venues,\n",
    "    embedding_dim=64,\n",
    "    mlp_dims=[128, 64, 32],\n",
    "    dropout_rate=0.3,\n",
    "    implicit_weight=0.5\n",
    ").to(device)\n",
    "\n",
    "# Setup optimizer and loss functions\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "explicit_criterion = nn.MSELoss()\n",
    "implicit_criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b74920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 2.8684, Val RMSE: 1.3985, LR: 0.001000\n",
      "Model improved - saving checkpoint (RMSE: 1.3985)\n",
      "Epoch 2/20 - Train Loss: 1.7543, Val RMSE: 1.1522, LR: 0.001000\n",
      "Model improved - saving checkpoint (RMSE: 1.1522)\n",
      "Epoch 3/20 - Train Loss: 0.9190, Val RMSE: 1.1103, LR: 0.001000\n",
      "Model improved - saving checkpoint (RMSE: 1.1103)\n",
      "Epoch 4/20 - Train Loss: 0.4465, Val RMSE: 0.9542, LR: 0.001000\n",
      "Model improved - saving checkpoint (RMSE: 0.9542)\n",
      "Epoch 5/20 - Train Loss: 0.3090, Val RMSE: 0.9831, LR: 0.001000\n",
      "No improvement for 1 epochs\n",
      "Epoch 6/20 - Train Loss: 0.2627, Val RMSE: 1.0113, LR: 0.001000\n",
      "No improvement for 2 epochs\n",
      "Epoch 7/20 - Train Loss: 0.2300, Val RMSE: 1.0375, LR: 0.001000\n",
      "No improvement for 3 epochs\n",
      "Epoch 8/20 - Train Loss: 0.2048, Val RMSE: 1.0594, LR: 0.000500\n",
      "No improvement for 4 epochs\n",
      "Epoch 9/20 - Train Loss: 0.1559, Val RMSE: 1.0845, LR: 0.000500\n",
      "No improvement for 5 epochs\n",
      "Early stopping triggered after 9 epochs\n",
      "Training complete! Best RMSE: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2398/576885734.py:812: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_implicit_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model, best_rmse = train_implicit_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    explicit_criterion=explicit_criterion,\n",
    "    implicit_criterion=implicit_criterion,\n",
    "    epochs=20,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "print(f\"Training complete! Best RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = create_ensemble(num_users, num_items, device)\n",
    "        \n",
    "# Train each model\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"\\nTraining model {i+1}/{len(models)}\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    train_enhanced(\n",
    "        model=model,\n",
    "        train_df=train_df_norm,\n",
    "        valid_df=valid_df_norm,\n",
    "        loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=15,\n",
    "        patience=3\n",
    "    )\n",
    "\n",
    "# Create ensemble prediction function\n",
    "def ensemble_pred_fn(sids, pids):\n",
    "    return ensemble_predict(models, sids, pids, device)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_rmse = evaluate(valid_df, ensemble_pred_fn)\n",
    "print(f\"Ensemble RMSE: {ensemble_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
